{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# GPT 언어 모델"
      ],
      "metadata": {
        "id": "I95xxe4n4Jw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT가 공개되었을 당시에는 굉장히 주목을 받았습니다.\n",
        "\n",
        "자연어 문장을 분류해주는 아주 성능 좋은 디코더!\n",
        "적은 양의 데이터로도 좋은 성능 발휘 가능\n",
        "다양한 자연어 태스크에서 최고 성능 달성\n",
        "등등 굉장히 많은 장점들을 가지고 있었습니다.\n",
        "\n",
        "하지만 당연히 아직 해결하지 못한 문제들 또한 남아 있었습니다.\n",
        "\n",
        "지도학습이 필수적 (라벨이 있는 데이터가 필요!)\n",
        "특정 태스크에 맞게 학습된 모델은 다른 태스크에서는 사용 불가"
      ],
      "metadata": {
        "id": "-t_kS1xv4L-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2 사전학습해보기"
      ],
      "metadata": {
        "id": "-5v36_1V6BER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPNKSgpv6d9u",
        "outputId": "80f8d2a6-09aa-43b8-8c09-0e46e8e70935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THtxuhMz6mN2",
        "outputId": "950f39c4-663f-4009-cba9-45efffadb6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHbUZ5co4EaE",
        "outputId": "972a14cb-05e6-4d46-c734-2ab866cc7f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 1323k  100 1323k    0     0  1726k      0 --:--:-- --:--:-- --:--:-- 1726k\n"
          ]
        }
      ],
      "source": [
        "!mkdir my_data\n",
        "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
        "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt\n",
        "\n",
        "path = \"/content/my_data/wiki_20190620_small.txt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from tokenizers.normalizers import BertNormalizer\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer()\n",
        "\n",
        "tokenizer._tokenizer.normalizer = BertNormalizer(clean_text=True,\n",
        "handle_chinese_chars=False,\n",
        "lowercase=False)\n",
        "\n",
        "tokenizer.train(\n",
        "    path,\n",
        "    vocab_size=10000,\n",
        "    special_tokens=[\n",
        "        \"<s>\", # 문장의 시작\n",
        "        \"<pad>\", # 패딩 토큰\n",
        "        \"</s>\", # 문장의 끝\n",
        "        \"<unk>\", # 사전에 없는 토큰\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "v8du7btB6FyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n",
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n",
        "print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))\n",
        "# SentencePiece를 사용하면, 나중에 decoding 과정에서 '_' 만 ' '로 replace해주면 띄어쓰기 복원이 가능해집니다."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hw0LmxzV6FvR",
        "outputId": "505bfef2-46f5-4a53-9ffa-6a944cb4a0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1005, 577, 6613, 1303, 1041, 2071, 1136, 594, 1033]\n",
            "['▁이', '순', '신은', '▁조선', '▁중', '기의', '▁무', '신', '이다.']\n",
            "이순신은 조선 중기의 무신이다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_model('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeWEVF2P7TFx",
        "outputId": "7fd912ea-f872-48fa-fbd8-0fe7a2418474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vocab.json', './merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer= SentencePieceBPETokenizer.from_file(vocab_filename='vocab.json', meges_filename=\"merges.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "UaNhdXHv7RSj",
        "outputId": "81753fc0-9267-4c62-ee79-dea6af8b1b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vocab_filename' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-fb03493a7f31>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#####뒤에뭘까\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mSentencePieceBPETokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_filename\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m'vocab.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeges_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"merges.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab_filename' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\"))\n",
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n",
        "print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n",
        "print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n",
        "print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "afCBq9cM7k2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<shkim>\"])\n",
        "tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n",
        "tokenizer.unk_token_id = tokenizer.token_to_id(\"<unk>\")\n",
        "tokenizer.bos_token_id = tokenizer.token_to_id(\"<bos>\")\n",
        "tokenizer.eos_token_id = tokenizer.token_to_id(\"<eos>\")\n",
        "\n",
        "print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids)\n",
        "print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n",
        "print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "I59FHBD88Boe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer.get_vocab_size(),\n",
        "  bos_token_id=tokenizer.token_to_id(\"<s>\"),\n",
        "  eos_token_id=tokenizer.token_to_id(\"</s>\"),\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)"
      ],
      "metadata": {
        "id": "iPP4SICl6FqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.num_parameters()"
      ],
      "metadata": {
        "id": "pik-BgkG8dpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "import warnings\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "from filelock import FileLock\n",
        "\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer\n",
        "from transformers.utils import logging"
      ],
      "metadata": {
        "id": "epxF10cB6SAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This will be superseded by a framework-agnostic approach soon.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: PreTrainedTokenizer,\n",
        "        file_path: str,\n",
        "        block_size: int,\n",
        "        overwrite_cache=False,\n",
        "        cache_dir: Optional[str] = None,\n",
        "    ):\n",
        "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
        "\n",
        "        block_size = block_size - tokenizer.num_special_tokens_to_add(is_pair=False)\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            cache_dir if cache_dir is not None else directory,\n",
        "            \"cached_lm_{}_{}_{}\".format(\n",
        "                tokenizer.__class__.__name__,\n",
        "                str(block_size),\n",
        "                filename,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Make sure only the first process in distributed training processes the dataset,\n",
        "        # and the others will use the cache.\n",
        "        lock_path = cached_features_file + \".lock\"\n",
        "        with FileLock(lock_path):\n",
        "\n",
        "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"rb\") as handle:\n",
        "                    self.examples = pickle.load(handle)\n",
        "                logger.info(\n",
        "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
        "                # 여기서부터 본격적으로 데이터셋을 만들기 시작합니다.\n",
        "                self.examples = []\n",
        "                text = \"\"\n",
        "                with open(file_path, encoding=\"utf-8\") as f: # file_path가 path 말하는거임\n",
        "                    lines = f.readlines()\n",
        "                    for line in lines:\n",
        "                        line = line.strip()\n",
        "                        line = \"<s>\"+line+\"</s>\" # 학습 데이터 앞 뒤에 문장 구분 기호를 추가해줍니다.\n",
        "                        text += line    # 'text' 객체에 모든 학습 데이터를 다 합쳐버립니다 :-)\n",
        "                tokenized_text = tokenizer.encode(text).ids\n",
        "\n",
        "                # 모델의 최대 sequence length만큼 데이터를 잘라서 저장합니다.\n",
        "                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
        "                    self.examples.append(\n",
        "                        tokenized_text[i : i + block_size]\n",
        "                    )\n",
        "                # Note that we are losing the last truncated example here for the sake of simplicity (no padding)\n",
        "                # If your dataset is small, first you should look for a bigger one :-) and second you\n",
        "                # can change this behavior by adding (model specific) padding.\n",
        "\n",
        "                start = time.time()\n",
        "                with open(cached_features_file, \"wb\") as handle:\n",
        "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "                logger.info(\n",
        "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
        "                )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i) -> torch.Tensor:\n",
        "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
        "\n",
        "dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=path,\n",
        "    block_size=128,\n",
        ")"
      ],
      "metadata": {
        "id": "2pChERPb6R97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(    # GPT는 생성모델이기 때문에 [MASK] 가 필요 없습니다 :-)\n",
        "    tokenizer=tokenizer, mlm=False,)"
      ],
      "metadata": {
        "id": "DZS3Wa7I6VD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "VYr6PBob9T5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='model_output',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=64, # 512:32  # 128:64\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Sy_2JZwO6U9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "rSbuXRUp9cyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = 1\n",
        "device= torch.device('cuda' if (torch.cuda.is_available() and USE_GPU) else 'cpu')"
      ],
      "metadata": {
        "id": "XWkgeoRC9e7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode(\"<s>이순신\", add_special_tokens=True).ids).unsqueeze(0).to('cuda')\n",
        "\n",
        "output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=100, num_return_sequences=3)\n",
        "for generated_sequence in output_sequences:\n",
        "    generated_sequence = generated_sequence.tolist()\n",
        "    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "UQPqOxNn6R7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 자연어 생성 직접해보기"
      ],
      "metadata": {
        "id": "jCjfzQe29nzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoGPT-2모델 사용\n",
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!apt-get install git-lfs\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/taeminlee/kogpt2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QL3lowLL9oOZ",
        "outputId": "b1fa30f0-17e5-49dd-9b6d-cc37522794ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected operating system as Ubuntu/jammy.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Detected apt version as 2.4.12\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... Packagecloud gpg key imported to /etc/apt/keyrings/github_git-lfs-archive-keyring.gpg\n",
            "done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following packages will be upgraded:\n",
            "  git-lfs\n",
            "1 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 7,420 kB of archives.\n",
            "After this operation, 6,051 kB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu jammy/main amd64 git-lfs amd64 3.5.1 [7,420 kB]\n",
            "Fetched 7,420 kB in 1s (12.2 MB/s)\n",
            "(Reading database ... 121757 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_3.5.1_amd64.deb ...\n",
            "Unpacking git-lfs (3.5.1) over (3.0.2-1ubuntu0.2) ...\n",
            "Setting up git-lfs (3.5.1) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "\n",
            "Exiting because of \"interrupt\" signal.\n",
            "^C\n",
            "Cloning into 'kogpt2'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Total 56 (delta 0), reused 0 (delta 0), pack-reused 56\u001b[K\n",
            "Unpacking objects: 100% (56/56), 1.53 MiB | 3.09 MiB/s, done.\n",
            "Filtering content: 100% (3/3), 1.41 GiB | 26.27 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer(\"/content/kogpt2/vocab.json\", \"/content/kogpt2/merges.txt\")\n",
        "\n",
        "config = GPT2Config(vocab_size=50000)\n",
        "config.pad_token_id = tokenizer.token_to_id('<pad>')\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "model_dir = '/content/kogpt2/pytorch_model.bin'\n",
        "\n",
        "model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n",
        "model.to('cuda')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "37KxEjQ8Aoyi",
        "outputId": "a4e1e7aa-9d60-42ce-8dbd-3a16894d9d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-82446dae5b71>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/kogpt2/pytorch_model.bin'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1266\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m                 typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1205\u001b[0;31m                     \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1206\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                     _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1308\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    251\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "tokenizer.add_special_tokens([\"<s>\", \"</s>\"])"
      ],
      "metadata": {
        "id": "I7Ow4q0aSCnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizing(text):\n",
        "    return torch.tensor(tokenizer.encode('<s> '+text, add_special_tokens=False).ids).unsqueeze(0).to('cuda')\n",
        "\n",
        "input_ids = tokenizing(\"이순신은 조선 중기의 무신이다.\")\n",
        "\n",
        "# generate text until the output length (which includes the context length) reaches 100\n",
        "# 생성 모델은 generate 함수를 통해 다음 token을 생성해낼 수 있습니다.\n",
        "greedy_output = model.generate(input_ids, max_length=100)\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(greedy_output.tolist()[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "ALSI2rg6AowC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# beam_sesarch\n",
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "n-Mq8uPHAotb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_output = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "metadata": {
        "id": "us4ZQ748Aoq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pritn('Output:\\n' + 100*'-')\n",
        "print(tokenizer.decode(beam_output.tolist()[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "EuBcT2EPSppr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    num_return_sequences=5,\n",
        "    early_stopping=True\n",
        ")"
      ],
      "metadata": {
        "id": "j1jJ2qvUAu3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pritn('Output:\\n' + 100*'-')\n",
        "for i, beam_output in enumerate(beam_outputs):\n",
        "  print('{}: {}'.format(i, tokenizer.decode(beam_output.tolist(), skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "5TWGKwd9S3mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sampling\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, # 완전 random sampling\n",
        "    max_length=50,\n",
        "    top_k=0 # w/o top_k 추출\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "i1Eh9sEAAuwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_k=0,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "zVtKA9XsAyP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-K Sampling\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_k=50\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "LClGoDkjA37N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-p(nucleus) Sampling\n",
        "sample_output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    max_length=50,\n",
        "    top_p=0.92,\n",
        "    top_k=0\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "print(tokenizer.decode(sample_output.tolist()[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "yi_LEstnA34w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few-shot 학습 및 koGPT 사용해보기"
      ],
      "metadata": {
        "id": "Ka_8jeWlTqcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!apt-get install git-lfs\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/taeminlee/kogpt2"
      ],
      "metadata": {
        "id": "luytGTetT1eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer(\"/content/kogpt2/vocab.json\", \"/content/kogpt2/merges.txt\")\n",
        "\n",
        "config = GPT2Config(vocab_size=50000)\n",
        "config.pad_token_id = tokenizer.token_to_id('<pad>')\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "model_dir = '/content/kogpt2/pytorch_model.bin'\n",
        "\n",
        "model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n",
        "model.to('cuda')\n",
        "\n"
      ],
      "metadata": {
        "id": "uLbcpW5MT3mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = tokenizer.encode('이순신은 조선 중기의 무신이다.', add_special_tokens=True)\n",
        "print(tokenized_text)\n",
        "print(tokenized_text.tokens)\n",
        "print(tokenized_text.ids)"
      ],
      "metadata": {
        "id": "PsSrgPCSUaPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.add_special_tokens([\"<s>\", \"</s>\"])"
      ],
      "metadata": {
        "id": "h0v8KcIrUXTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############뒤에짤림\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode('이순신은', add_special_tokens=True).ids).unsqueeze(0).)\n",
        "\n",
        "output~~~"
      ],
      "metadata": {
        "id": "WRYqgOzFUlcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpt_output(input_sent):\n",
        "    input_ids = torch.tensor(tokenizer.encode(input_sent, add_special_tokens=True).ids).unsqueeze(0).to('cuda')\n",
        "    sample_outputs = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        max_length=512,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.token_to_id(\"</s>\"),\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    generated_sequence = sample_outputs[0].tolist()\n",
        "    return tokenizer.decode(generated_sequence, skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "C0dSfikNT3j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_gpt_output('<s>이순신은')"
      ],
      "metadata": {
        "id": "XkhLo8nyU7Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_gpt_output(\"<s>철수 : 영희야 안녕!</s><s>영희 : \")"
      ],
      "metadata": {
        "id": "aWkKy0GsVABZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_gpt_output(\"<s>철수 : 영희야 안녕!</s><s>영희 : 어! 철수야! 오랜만이다!</s><s>철수 : 그러게~ 잘 지냈어?</s><s>영희 : \")"
      ],
      "metadata": {
        "id": "QvyfcJyRT3hK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_gpt_output(\"<s>본문 : 아.. 기분 진짜 짜증나네ㅡㅡ</s><s>감정 : 분노</s><s>본문 : 와!! 진짜 너무 좋아!!</s><s>감정 : \")"
      ],
      "metadata": {
        "id": "7g5fTaqYT3c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_gpt_output(\"<s>질문 : 코로나 바이러스에 걸리면 어떻게 되나요?</s>\\\n",
        "<s>답 : COVID-19 환자는 일반적으로 감염 후 평균 5 ~ 6 일 (평균 잠복기 5 ~ 6 일, 범위 1 ~ 14 일)에 경미한 호흡기 증상 및 발열을 포함한 징후와 증상을 나타냅니다. COVID-19 바이러스에 감염된 대부분의 사람들은 경미한 질병을 앓고 회복됩니다.</s>\\\n",
        "<s>질문 : 코로나 바이러스 질병의 첫 증상은 무엇입니까?</s>\\\n",
        "<s>답 : 이 바이러스는 경미한 질병에서 폐렴에 이르기까지 다양한 증상을 유발할 수 있습니다. 질병의 증상은 발열, 기침, 인후통 및 두통입니다. 심한 경우 호흡 곤란과 사망이 발생할 수 있습니다.</s>\\\n",
        "<s>질문 : 딸기 식물의 수명주기는 무엇입니까?</s>\\\n",
        "<s>답 : 딸기의 생애는 새로운 식물의 설립으로 시작하여 2 ~ 3 년 후 절정에 이르렀다가 절정에 이어 2 ~ 3 년에 노화와 죽음을 향해 진행됩니다. 이상적인 조건에서 딸기 식물은 5-6 년까지 살 수 있습니다.</s>\\\n",
        "<s>질문 : 파이썬 메서드의 self 매개 변수의 목적은 무엇입니까?</s>\\\n",
        "<s>답 : self 매개 변수는 클래스의 현재 인스턴스에 대한 참조이며 클래스에 속한 변수에 액세스하는 데 사용됩니다.</s>\\\n",
        "<s>질문 : 뇌의 어떤 부분이 말을 제어합니까?</s>\\\n",
        "<s>답 : 언어 우세 반구의 왼쪽 전두엽 (브로카 영역)에있는 뇌의 분리 된 부분에 대한 손상은 자발적 언어 및 운동 언어 제어 사용에 상당한 영향을 미치는 것으로 나타났습니다.</s>\\\n",
        "<s>질문 : 인공지능의 미래에 대해 어떻게 생각하십니까?</s>\\\n",
        "<s>답 : \")"
      ],
      "metadata": {
        "id": "qmmncGpDT3aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_gpt_output(\"<s>한국어: 그 도로는 강과 평행으로 뻗어 있다.</s>\\\n",
        "<s>English: The road runs parallel to the river.</s>\\\n",
        "<s>한국어: 그 평행선들은 분기하는 것처럼 보인다.</s>\\\n",
        "<s>English: The parallel lines appear to diverge.</s>\\\n",
        "<s>한국어: 그 도로와 운하는 서로 평행하다.</s>\\\n",
        "<s>English: The road and the canal are parallel to each other.</s>\\\n",
        "<s>한국어: 평행한 은하계라는 개념은 이해하기가 힘들다.</s>\\\n",
        "<s>English: The idea of a parallel universe is hard to grasp.</s>\\\n",
        "<s>한국어: 이러한 전통은 우리 문화에서는 그에 상응하는 것이 없다.</s>\\\n",
        "<s>English: This tradition has no parallel in our culture.</s>\\\n",
        "<s>한국어: 이것은 현대에 들어서는 그 유례를 찾기 힘든 업적이다.</s>\\\n",
        "<s>English: This is an achievement without parallel in modern times.</s>\\\n",
        "<s>한국어: 그들의 경험과 우리 경험 사이에서 유사점을 찾는 것이 가능하다.</s>\\\n",
        "<s>English: It is possible to draw a parallel between their experience and ours.</s>\\\n",
        "<s>한국어: 그 새 학위 과정과 기존의 수료 과정이 동시에 운영될 수도 있을 것이다.</s>\\\n",
        "<s>English: The new degree and the existing certificate courses would run in parallel.</s>\\\n",
        "<s>한국어: 이순신은 조선 중기의 무신이다.</s>\\\n",
        "<s>Englisth: \")"
      ],
      "metadata": {
        "id": "JXChv6vGT3Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 언어모델 학습 및 다중 과제 튜닝\n",
        "# koGPT-2 기반의 챗봇 실습\n",
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!apt-get install git-lfs\n"
      ],
      "metadata": {
        "id": "O1U4E3ooT3Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/taeminlee/kogpt2\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "ORlopGF9Vmtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tokenizers import SentencePieceBPETokenizer\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = SentencePieceBPETokenizer(\"/content/kogpt2/vocab.json\", \"/content/kogpt2/merges.txt\")\n",
        "\n",
        "config = GPT2Config(vocab_size=50000)\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "model_dir = '/content/kogpt2/pytorch_model.bin'\n",
        "\n",
        "model.load_state_dict(torch.load(model_dir, map_location='cuda'), strict=False)\n",
        "model.to('cuda')\n"
      ],
      "metadata": {
        "id": "Pk2FNK4zUBQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_text = tokenizer.encode('이순신은 조선 중기의 무신이다.', add_special_tokens=True)\n",
        "print(tokenized_text)\n",
        "print(tokenized_text.tokens)\n",
        "print(tokenized_text.ids)"
      ],
      "metadata": {
        "id": "7KaXsG2hVr4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############뒤에짤림\n",
        "import torch\n",
        "torch.manual_seed(42)\n",
        "\n",
        "input_ids = torch.tensor(tokenizer.encode('이순신은', add_special_tokens=True).ids).unsqueeze(0).)\n",
        "\n",
        "output~~~"
      ],
      "metadata": {
        "id": "BXyy7yOSVxFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 준비\n",
        "!git clone https://github.com/songys/Chatbot_data.git\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/Chatbot_data/ChatbotData.csv')\n",
        "\n",
        "data.head(10)\n"
      ],
      "metadata": {
        "id": "yQw8WQVJUBN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "added_special_token_num = tokenizer.add_special_tokens(['<s>', '</s>'])\n",
        "\n",
        "print(added_special_token_num)"
      ],
      "metadata": {
        "id": "NJiGf_BDV5WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad_id = tokenizer.token_to_id(\"<pad>\")\n",
        "print(pad_id)\n",
        "tokenizer.enable_padding(pad_id=pad_id, pad_token=\"<pad>\")\n",
        "tokenizer.enable_truncation(max_length=128)"
      ],
      "metadata": {
        "id": "LEitY64pUBLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenizer, file_path):\n",
        "        self.data = []\n",
        "        self.file_path = file_path\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def load_data(self):\n",
        "        raw_data = pd.read_csv(self.file_path)\n",
        "        train_data = '<s>'+raw_data['Q']+'</s>'+'<s>'+raw_data['A']+'</s>'\n",
        "        #<s>안녕하세요</s><s> -> 네, 안녕하세요</s>\n",
        "        tokenized_train_data = tokenizer.encode_batch(train_data)\n",
        "        for single_data in tokenized_train_data:\n",
        "            self.data.append(torch.tensor(single_data.ids).unsqueeze(0))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        item = self.data[index]\n",
        "        return item\n",
        "\n",
        "train_dataset = ChatDataset(tokenizer=tokenizer, file_path='/content/Chatbot_data/ChatbotData.csv')\n",
        "train_dataset.load_data()"
      ],
      "metadata": {
        "id": "R--aFXnSUBIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
      ],
      "metadata": {
        "id": "E5B4QCZkUBF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW"
      ],
      "metadata": {
        "id": "8H6C_idbWe-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=1e-4, correct_bias=True)\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "avg_loss = (0.0, 0.0)\n",
        "for epoch in range(epochs):\n",
        "    count=0\n",
        "    for data in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        data = data.transpose(1,0)\n",
        "        data = data.to('cuda')\n",
        "        model = model.to('cuda')\n",
        "\n",
        "        outputs = model(data, labels=data)\n",
        "        loss, logits = outputs[:2]\n",
        "        loss = loss.to('cuda')\n",
        "        loss.backward()\n",
        "        avg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
        "        optimizer.step()\n",
        "        if count % 200 == 0:\n",
        "            print('epoch no.{0}  train ({1}/{2})  loss = {3:.5f}  avg_loss = {4:.5f}' . format(epoch, count, len(data_loader), loss, avg_loss[0] / avg_loss[1]))\n",
        "        count += 1"
      ],
      "metadata": {
        "id": "-MtGo-XAUHui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'chitchat_model.bin')"
      ],
      "metadata": {
        "id": "f41Fx01_UHr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding(text):\n",
        "    text = '<s>'+text+'</s><s>'\n",
        "    return torch.tensor(tokenizer.encode(text).ids).unsqueeze(0).to('cuda')\n",
        "\n",
        "def decoding(ids):\n",
        "    return tokenizer.decode_batch(ids)\n",
        "\n",
        "tokenizer.no_padding()\n",
        "tokenizer.no_truncation()\n",
        "\n",
        "e_s = tokenizer.token_to_id('</s>')\n",
        "unk = tokenizer.token_to_id('<unk>')"
      ],
      "metadata": {
        "id": "4Zttmhb_UHpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(input_sent):\n",
        "    input_ids = encoding(input_sent)\n",
        "\n",
        "    sample_outputs = model.generate(\n",
        "        input_ids,\n",
        "        num_return_sequences=5,\n",
        "        do_sample=True,\n",
        "        max_length=128,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        eos_token_id=e_s,\n",
        "        early_stopping=True,\n",
        "        bad_words_ids=[[unk]]\n",
        "    )\n",
        "\n",
        "    decoded_result = decoding(sample_outputs.tolist())\n",
        "    for result in decoded_result:\n",
        "        print(result)"
      ],
      "metadata": {
        "id": "cr4xcOaeUBDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_answer('안녕?')"
      ],
      "metadata": {
        "id": "ZNojnM0-UMdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_answer('만나서 반가워.')"
      ],
      "metadata": {
        "id": "BmV7e82fUOHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_answer('인공지능의 미래에 대해 어떻게 생각하세요?')"
      ],
      "metadata": {
        "id": "kOaXeZUQUPhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 트랜스포머 기반 언어 모델 연대기"
      ],
      "metadata": {
        "id": "Px31-EmmXBAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 멀티 모달"
      ],
      "metadata": {
        "id": "SCVEWSm3XNzb"
      }
    }
  ]
}