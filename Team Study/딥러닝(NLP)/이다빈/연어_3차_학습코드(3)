{"cells":[{"cell_type":"markdown","metadata":{"id":"6PQSnsGeA2OH"},"source":["# 트랜스포머 (Transformer)\n","\n","* 참고: https://wikidocs.net/31379"]},{"cell_type":"markdown","metadata":{"id":"nbQ-h_XxBAiq"},"source":["\n","---\n","* 어텐션 메커니즘은 입력 시퀀스의 각 단어에 대한 중요도(가중치)를 동적으로 계산하여 출력에 반영하는 방법\n","\n","---\n","\n","\n","* attention mechanism은 seq2seq의 입력 시퀀스 정보 손실을 보정해주기 위해 사용됨\n","* attention mechanism을 보정 목적이 아닌, 인코더와 디코더로 구성한 모델이 바로 트랜스포머\n","* 트랜스포머는 RNN을 사용하지 않고 인코더와 디코더를 설계하였으며, 성능도 RNN보다 우수함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RDiFPIdUBBS2"},"source":["## 포지셔널 인코딩"]},{"cell_type":"markdown","metadata":{"id":"rLqHf_4SEWoa"},"source":["* 기존의 RNN은 단어의 위치를 따라 순차적으로 입력받아 단어의 위치정보를 활용할 수 있었음\n","* 트랜스포머의 경우, RNN을 활용하지 않았기 때문에 단어의 위치정보를 다른 방식으로 줄 필요가 있음\n","* 이를 위해 **각 단어의 임베딩 벡터에 위치 정보들을 더하게 되는데** 이를 포지셔널 인코딩이라 함\n","* 보통 포지셔널 인코딩은 sin, cos을 이용하여 계산"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"SiO5c_HIFBAk","executionInfo":{"status":"ok","timestamp":1711880985078,"user_tz":-540,"elapsed":19,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["# 트랜스포머 모델에서 사용되는 포지셔널 인코딩을 생성\n","\n","def positional_encoding(dim,sentence_length): # 임베딩 차원(dim)과 문장의 길이(sentence_length)를 입력으로 받는다.\n","  encoded_vec = np.array([pos / np.power(10000, 2*i / dim) for pos in range(sentence_length) for i in range(dim)]) # 입력 문장의 각 위치와 임베딩 차원에 대한 포지셔널 인코딩 값을 계산\n","  # pos : 몇 번째 해당하는 위치인지(위치값) 알려줌\n","\n","  # 포지셔널 인코딩 값을 사인 함수와 코사인 함수를 사용하여 각각 짝수 인덱스와 홀수 인덱스에 대해 변경\n","  encoded_vec[::2] = np.sin(encoded_vec[::2]) # 짝수는 sin, -1~1 사이로\n","  encoded_vec[1::2] = np.cos(encoded_vec[1::2]) # 홀수는 cos, -1~1 사이로\n","  return tf.constant(encoded_vec.reshape([sentence_length, dim]), dtype=tf.float32) # 반환된 포지셔널 인코딩은 TensorFlow의 상수 텐서로 반환"]},{"cell_type":"markdown","metadata":{"id":"099gUUxhAgy3"},"source":["## 레이어 정규화"]},{"cell_type":"markdown","metadata":{"id":"XCdips98yPuH"},"source":["*  레이어 정규화에서는 텐서의 마지막 차원에 대해 평균과 분산을 구하고, 이 값을 통해 값을 정규화함\n","*  해당 정규화를 각 층의 연결에 편리하게 적용하기 위해 함수화한 `sublayer_connection()`을 선언"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TSJjxF86Aeg3","executionInfo":{"status":"ok","timestamp":1711880985080,"user_tz":-540,"elapsed":17,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def layer_norm(inputs, eps=1e-6):\n","  feature_shape = inputs.get_shape()[-1:] # 입력 텐서의 마지막 차원의 크기를 획득\n","  mean = tf.keras.backend.mean(inputs, [-1], keepdims = True) # 입력 텐서의 각 요소에 대한 평균과 표준 편차를 계산\n","  std = tf.keras.backend.std(inputs, [-1], keepdims= True)\n","  beta = tf.Variable(tf.zeros(feature_shape), trainable = False)\n","  gamma = tf.Variable(tf.ones(feature_shape), trainable=False)\n","  # 레이어 정규화를 위한 학습 가능한 변수인 beta와 gamma를 초기화, beta는 오프셋(offset), gamma는 스케일(scale) 매개변수\n","  # 입력을 평균과 표준 편차로 정규화하고, 학습 가능한 beta와 gamma를 사용하여 조정\n","  return gamma * (inputs - mean) / (std + eps) + beta"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"km9ORxIun-MU","executionInfo":{"status":"ok","timestamp":1711880985080,"user_tz":-540,"elapsed":16,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def sublayer_connection(inputs, sublayer, dropout=0.2): # 드롭아웃 적용: 입력과 서브레이어의 출력에 드롭아웃을 적용. 이를 통해 모델의 일반화 성능을 향상시키고 과적합을 방지\n","  outputs = layer_norm(inputs + tf.keras.layers.Dropout(dropout)(sublayer))\n","  # 입력과 서브레이어의 출력을 더함. 이를 통해 서브레이어의 출력에 입력을 추가하여 정보 흐름을 유지하고, 그래디언트 소실 문제를 완화\n","  # 레이어 정규화: 서브레이어 커넥션을 거친 결과에 레이어 정규화를 적용. 이를 통해 모델의 학습을 안정화하고, 훈련 과정을 더 잘 수렴시킴.\n","  return outputs\n","\n","# 서브레이어 커넥션을 구현하여 입력과 서브레이어의 출력을 더하고, 레이어 정규화를 적용하여 반환"]},{"cell_type":"markdown","metadata":{"id":"Ppb7IxJ3diMC"},"source":["## 어텐션"]},{"cell_type":"markdown","metadata":{"id":"1JaU6MHgy9V2"},"source":["\n","\n","*   트랜스포머 모델의 핵심이 되는 부분\n","*   트랜스포머에서는 multi-head attention과 self attention이라는 개념을 사용\n","  1.   multi-head attention\n","      * 디코더가 가지는 차원을 나누어 병렬로 어텐션을 진행\n","      *  마지막엔 병렬로 각 진행해 얻은 어텐션 헤드를 모두 연결\n","      * 이로 인해 다양한 시각에서 정보를 수집할 수 있는 효과를 얻음\n","  2.   self attention\n","      *   일반적인 어텐션의 경우, 특정 시점의 디코더 은닉상태와 모든 시점의 인코더 은닉상태를 활용\n","      *   이는 입력 문장과 다른 문장에 존재하는 단어간의 어텐션을 의미함\n","      *   반면 self attention은 은닉 상태를 동일하게 하여 어텐션을 진행\n","      *   이는 입력 문장 내 단어간의 어텐션을 의미함\n","\n","\n","\n","\n","*   트랜스포머 제안 논문에서는 scaled-dot product attention을 활용해 모델을 작성함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kRyL0KDXi6ej"},"source":["### scaled-dot product attention 구현"]},{"cell_type":"markdown","metadata":{"id":"6HtmcgRR3Cr-"},"source":["* scaled-dot product attention은 앞서 학습한 dot product attention과 거의 유사함\n","* 단 attention을 진행할 때 어텐션 스코어를 계산할 때 내적 값을 정규화\n","* 트랜스포머에서는 정규화할 때 K 벡터(=디코더 셀의 은닉 상태)의 차원을 루트를 취한 값을 사용"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ALEMzi4fdiSQ","executionInfo":{"status":"ok","timestamp":1711880985081,"user_tz":-540,"elapsed":17,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def scaled_dot_product_attention(query, key, value, masked=False):\n","  key_dim_size = float(key.get_shape().as_list()[-1])\n","  key = tf.transpose(key, perm=[0,2,1])\n","\n","  outputs = tf.matmul(query, key)/ tf.sqrt(key_dim_size)\n","\n","  if masked:\n","    diag_vals = tf.ones_like(outputs[0, :, :])\n","    tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()\n","    masks = tf.tile(tf.expand_dims(tril,0), [tf.shape(outputs)[0],1,1])\n","    paddings = tf.ones_like(masks)*(-2**30)\n","    outputs = tf.where(tf.equal(masks, 0), paddings, outputs)\n","\n","    attention_map = tf.nn.softmax(outputs)\n","    return tf.matmul(attention_map, value)\n","\n","# 주어진 쿼리, 키, 밸류에 대해 scaled_dot_product_attention을 계산하고, 선택적으로 마스킹을 수행하여 어텐션 가중치를 반환\n","\n","# 1. 키 행렬 전치: 입력으로 주어진 키(Key) 텐서를 전치하여 행과 열을 교환. 이는 행렬 곱셈을 위한 준비 작업.\n","# 2. 스케일드 닷-프로덕트 계산: 쿼리(Query)와 전치된 키(Key)의 내적을 계산하고, 키 텐서의 차원 크기의 제곱근으로 나누어 스케일링. 이를 통해 어텐션 점수를 계산.\n","# 3. 마스킹: 선택적으로 마스킹을 수행. 만약 masked가 True로 설정되어 있다면, 다이아고날 마스크(diagonal mask)를 적용하여 현재 시점 이후의 어텐션을 제한.\n","#    이를 통해 모델이 현재 시점 이후의 정보를 참고하지 못하도록 함.\n","# 4. 소프트맥스 함수 적용: 계산된 어텐션 점수에 소프트맥스 함수를 적용하여 어텐션 가중치를 계산. 이를 통해 각 키(Key)에 대한 어텐션 가중치를 얻습니다.\n","# 5. 가중합 계산: 어텐션 가중치와 밸류(Value) 텐서를 사용하여 가중합을 계산. 이를 통해 쿼리에 대한 최종 어텐션 값을 얻습니다.\n","# 6. 결과 반환: 계산된 어텐션 값을 반환."]},{"cell_type":"markdown","metadata":{"id":"Yr20BxvVi-8b"},"source":["### multi-head attention 구현"]},{"cell_type":"markdown","metadata":{"id":"Gb5qflUH14-H"},"source":["* multi-head attention의 구현 과정\n","  1. query, key, value에 해당하는 값을 받고, 해당 값에 해당하는 행렬 생성\n","  2. 생성된 행렬들을 heads에 해당하는 수만큼 분리\n","  3. 분리한 행렬들에 대해 각각 어텐션을 수행\n","  4. 각 어텐션 결과들을 연결해 최종 어텐션 결과 생성\n","\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ooc3FAdQi_Gz","executionInfo":{"status":"ok","timestamp":1711880985081,"user_tz":-540,"elapsed":16,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def multi_head_attention(query, key, value, num_units, heads, masked=False):\n","  query = tf.keras.layers.Dense(num_units, activation = tf.nn.relu)(query)\n","  key = tf.keras.layers.Dense(num_units,activation=tf.nn.relu)(key)\n","  value = tf.keras.layers.Dense(num_units,activation=tf.nn.relu)(value)\n","\n","  query = tf.concat(tf.split(query, heads, axis=-1), axis =0)\n","  key = tf.concat(tf.split(key, heads, axis=-1), axis =0)\n","  value = tf.concat(tf.split(value, heads, axis=-1), axis =0) # 헤드 수만큼 분리하고 축 0으로 다시 concat(합침)\n","\n","  attention_map = scaled_dot_product_attention(query, key, value, masked)\n","  attn_outputs = tf.concat(tf.split(attention_map,heads, axis=0), axis=-1)\n","  attn_outputs = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(attn_outputs)\n","\n","  return attn_outputs"]},{"cell_type":"markdown","metadata":{"id":"QCcjAl77soBr"},"source":["\n","\n","---\n","\n","* 다중 헤드 어텐션(Multi-Head Attention)은 트랜스포머(Transformer) 모델에서 사용되는 어텐션 메커니즘 중 하나로, 단일 어텐션 헤드보다 더욱 풍부한 표현력을 제공하고자 만들어진 개념입니다. 다중 헤드 어텐션은 여러 개의 어텐션 헤드를 병렬로 실행하여 각각의 결과를 합치는 방식으로 작동\n","\n","<br>\n","<br>\n","\n","#### 다중 헤드 어텐션의 구조:\n","\n","###### 다중 헤드 어텐션은 크게 세 가지 단계로 이루어집니다: 선형 변환, 어텐션 계산, 다시 결합입니다. 이를 아래의 예제와 함께 살펴보겠습니다.\n","\n","* 선형 변환: 입력으로 주어진 쿼리(Query), 키(Key), 밸류(Value)를 각각 다른 가중치 행렬에 곱해 새로운 공간으로 선형 변환합니다. 이 과정에서 입력의 차원을 나타내는 d_model을 헤드의 수로 나누어 여러 개의 헤드로 분리합니다.\n","\n","* 어텐션 계산: 분리된 각 헤드에 대해 어텐션 계산을 수행합니다. 각 헤드는 독립적으로 어텐션 메커니즘을 수행하므로, 병렬로 계산될 수 있습니다.\n","\n","* 다시 결합: 계산된 각 헤드의 어텐션 값을 다시 결합하여 최종 다중 헤드 어텐션 값을 얻습니다. 이 과정에서는 다시 한 번 가중치 행렬에 의해 선형 변환됩니다.\n","\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BEPKHQJwun-D"},"source":["\n","\n","---\n","\n","\n","1.   피처 맵 생성: 쿼리, 키, 밸류에 각각 다층 퍼셉트론(Multi-Layer Perceptron, MLP)을 적용하여 피처 맵을 생성합니다. 이 과정에서는 쿼리, 키, 밸류를 고차원 공간으로 매핑합니다.\n","2.   헤드 분할: 생성된 피처 맵을 여러 개의 헤드로 나누어 분할합니다. 이를 통해 각 헤드는 입력의 서로 다른 측면을 고려할 수 있습니다.\n","\n","3.  어텐션 계산: 각 헤드에 대해 독립적으로 어텐션 계산을 수행합니다. 이 과정에서는 앞서 구현한 scaled_dot_product_attention 함수를 사용하여 어텐션을 계산합니다.\n","\n","4.  헤드 결합: 계산된 각 헤드의 어텐션 값을 결합하여 최종 다중 헤드 어텐션 값을 얻습니다. 이를 위해 헤드 별로 계산된 어텐션 값을 연결(concatenate)합니다.\n","\n","5.  피처 맵 복원: 결합된 다중 헤드 어텐션 값을 다시 다층 퍼셉트론을 통해 고차원 공간으로 매핑합니다.\n","\n","6.  결과 반환: 최종 다중 헤드 어텐션 값을 반환합니다.\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"78Zn5-fYITD4"},"source":["## 포지션-와이즈 피드 포워드 신경망"]},{"cell_type":"markdown","metadata":{"id":"-xxeG2xvo3ZN"},"source":["\n","\n","*   multi-head attention의 결과인 행렬을 입력받아 연산\n","*   일반적인 완전 연결 신경망(Dense layer)를 사용\n","*   position-wise FFNN은 인코더와 디코더에 모두 존재\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"0tSFd5OaITJ0","executionInfo":{"status":"ok","timestamp":1711880985082,"user_tz":-540,"elapsed":17,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def feed_forward(inputs, num_units):\n","  feature_shape = inputs.get_shape()[-1]\n","  inner_layer = tf.keras.layers.Dense(num_units, activation=tf.nn.relu)(inputs)\n","  outputs = tf.keras.layers.Dense(feature_shape)(inner_layer)\n","  return outputs\n","  # 입력 피처에 대해 복잡한 비선형 변환을 수행하여 새로운 피처를 생성가능. 이러한 과정은 트랜스포머 모델에서 인코더나 디코더의 일부로 사용될 수 있다.\n","\n","# 1. 입력의 차원 확인: get_shape() 메서드를 사용하여 입력 피처의 차원을 확인. 이를 통해 입력 피처의 형태를 알 수 있다.\n","# 2. 내부 레이어 생성: tf.keras.layers.Dense를 사용하여 입력 피처에 다층 퍼셉트론을 적용하는 내부 레이어를 생성.\n","#    이 과정에서는 num_units 개의 뉴런을 가진 은닉층을 만들고, 활성화 함수로는 ReLU(Rectified Linear Unit)를 사용. 이를 통해 비선형성을 추가하고 모델의 표현력을 향상시킴.\n","# 3. 출력 생성: 내부 레이어의 출력을 사용하여 최종 출력을 생성. 이때 출력의 차원은 입력의 차원과 동일하게 설정됩니다. 따라서 입력과 동일한 형태의 피처를 출력으로 반환.\n","# 4. 결과 반환: 생성된 출력을 반환."]},{"cell_type":"markdown","metadata":{"id":"XuccViYgBK6v"},"source":["## 인코더\n"]},{"cell_type":"markdown","metadata":{"id":"tG3MH0n1JVLz"},"source":["* 인코더는 하나의 어텐션을 사용\n","  + encoder self-attention (multi-head self-attention과 동일)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"m5T0pzBoAnn3","executionInfo":{"status":"ok","timestamp":1711880985082,"user_tz":-540,"elapsed":16,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def encoder_module(inputs, model_dim, ffn_dim, heads):\n","  self_attn = sublayer_connection(inputs, multi_head_attention(inputs, inputs, inputs, model_dim, heads))\n","  outputs = sublayer_connetion(self_attn, feed_forward(self_attn, ffn_dim))\n","\n","# encoder_module 함수:\n","# 1. encoder_module 함수는 트랜스포머의 인코더 모듈을 구현.\n","# 2. 인코더 모듈은 두 개의 서브레이어(sub-layer)로 구성.\n","# 3. 첫 번째 서브레이어는 멀티 헤드 어텐션(Multi-Head Attention)을 수행.\n","#    입력으로 주어진 inputs에 대해 셀프 어텐션(self-attention)을 수행하여 쿼리, 키, 밸류를 생성하고, 이를 다시 셀프 어텐션의 입력으로 사용하여 어텐션을 계산.\n","#    이후 서브레이어 커넥션(sublayer connection)을 통해 입력과 어텐션 결과를 결합.\n","# 4. 두 번째 서브레이어는 피드포워드 네트워크(Feed Forward Network)를 수행.\n","#    이는 멀티 헤드 어텐션의 결과를 입력으로 받아서 새로운 피처를 생성하는 과정.\n","#    마찬가지로 서브레이어 커넥션을 통해 입력과 피드포워드 네트워크의 출력을 결합.\n","# 5. 함수는 두 번째 서브레이어의 결과를 반환.\n","\n","\n","\n","def encoder(inputs, model_dim, ffn_dim, heads, num_layers):\n","  outputs = inputs\n","  for i in range(num_layers):\n","    outputs = encoder_module(outputs, model_dim, ffn_dim, heads)\n","\n","# encoder 함수:\n","# 1. encoder 함수는 주어진 입력에 대해 여러 개의 인코더 모듈을 적용하여 인코딩을 수행\n","# 2. 입력으로 주어진 inputs를 num_layers 만큼 반복하여 인코더 모듈을 적용.\n","# 3. 각 인코더 모듈의 출력을 다음 인코더 모듈의 입력으로 사용.\n","# 4. 인코더 모듈들을 모두 적용한 후 최종 출력을 반환.\n","\n","  return outputs"]},{"cell_type":"markdown","metadata":{"id":"lcgHRcTEBQqg"},"source":["## 디코더"]},{"cell_type":"markdown","metadata":{"id":"cNj-6FLQwT4-"},"source":["* 디코더는 다음과 같은 구성의 반복으로 이루어짐\n","  1. masked decoder self-attention\n","  2. encoder-decoder attention\n","  3. position-wise FFNN\n","\n","* 디코더에서는 2종류의 어텐션을 사용\n","  1.   masked decoder self-attention\n","    *   디코더에서는 인코더와는 달리 순차적으로 결과를 만들어 내야하기 때문에 다른 어텐션 방법을 사용함\n","    *   디코더 예측 시점 이후의 위치에 attention을 할 수 없도록 masking 처리\n","    *   결국 예측 시점에서 예측은 미리 알고 있는 위치까지만의 결과에 의존\n","  2.   encoder-decoder attention\n","    *   앞서 설명한 multi-head attention과 동일\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"2B05wr7aARcT","executionInfo":{"status":"ok","timestamp":1711880985082,"user_tz":-540,"elapsed":15,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def decoder_module(inputs, encoder_outputs, model_dim, ffn_dim, heads):\n","  masked_self_attn = sublayer_connection(inputs,\n","                                         multi_head_attention(inputs, inputs, inputs,\n","                                                              model_dim, heads, masked=True))\n","  self_attn = sublayer_connection(masked_self_attn,\n","                                  multi_head_attention(masked_self_attn,\n","                                                       encoder_outputs,\n","                                                       encoder_outputs,\n","                                                       model_dim, heads))\n","  outputs = sublayer_connection(self_attn, feed_forward(self_attn, ffn_dim))\n","  return outputs\n","# decoder_module 함수:\n","# 1. decoder_module 함수는 트랜스포머의 디코더 모듈을 구현.\n","# 2. 디코더 모듈은 세 개의 서브레이어(sub-layer)로 구성.\n","# 3. 첫 번째 서브레이어는 마스킹된 멀티 헤드 어텐션(Masked Multi-Head Attention)을 수행.\n","#    입력으로 주어진 inputs에 대해 셀프 어텐션(self-attention)을 수행하고, 마스킹을 적용하여 현재 시점 이후의 정보를 참조하지 못하도록 합니다.\n","# 4. 두 번째 서브레이어는 인코더-디코더 어텐션(Encoder-Decoder Attention)을 수행.\n","#    첫 번째 서브레이어의 출력과 인코더의 출력(encoder_outputs)을 입력으로 받아 어텐션을 계산.\n","# 5. 세 번째 서브레이어는 피드포워드 네트워크(Feed Forward Network)를 수행. 인코더-디코더 어텐션의 결과를 입력으로 받아 새로운 피처를 생성.\n","# 6. 함수는 세 번째 서브레이어의 결과를 반환.\n","\n","\n","\n","def decoder(inputs, encoder_outputs, model_dim, ffn_dim, heads, num_layers):\n","  outputs = inputs\n","  for i in range(num_layers):\n","    outputs = decoder_module(outputs, encoder_outputs, model_dim, ffn_dim, heads)\n","# decoder 함수:\n","# 1. decoder 함수는 주어진 입력에 대해 여러 개의 디코더 모듈을 적용하여 디코딩을 수행.\n","# 2. 입력으로 주어진 inputs를 num_layers 만큼 반복하여 디코더 모듈을 적용.\n","# 3. 각 디코더 모듈의 출력을 다음 디코더 모듈의 입력으로 사용.\n","# 4. 디코더 모듈들을 모두 적용한 후 최종 출력을 반환.\n","\n","\n","  return outputs"]},{"cell_type":"markdown","metadata":{"id":"EtztlyUB1ERS"},"source":["## 트랜스포머를 활용한 챗봇"]},{"cell_type":"markdown","metadata":{"id":"6CGUIAzv6eWs"},"source":["### konlpy 라이브러리"]},{"cell_type":"markdown","metadata":{"id":"Ae0mHT49v5gy"},"source":["*    한글을 처리하기 위해 konlpy 라이브러리 설치"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14344,"status":"ok","timestamp":1711880999412,"user":{"displayName":"이다빈","userId":"05137797752462423239"},"user_tz":-540},"id":"U8yf75uG6hBW","outputId":"8372deac-293a-453d-9f92-10859560621c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.0)\n","Installing collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"]}],"source":["!pip install konlpy"]},{"cell_type":"markdown","metadata":{"id":"rUMXvK5H1G9H"},"source":["### 데이터 준비"]},{"cell_type":"markdown","metadata":{"id":"miXrjR316mNb"},"source":["* 처리에 필요한 각종 변수 선언\n","* filters에 해당되는 문자를 걸러주는 정규 표현식 컴파일\n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"SMjn5PfE1GZR","executionInfo":{"status":"ok","timestamp":1711881005322,"user_tz":-540,"elapsed":5917,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["import re\n","import tensorflow as tf\n","\n","filters = \"([~.,!?\\\"':;)(])\"\n","PAD = '<PADDING>'\n","STD = '<START>'\n","END = '<END>'\n","UNK = '<UNKNOWN>'\n","\n","PAD_INDEX = 0\n","STD_INDEX = 1\n","END_INDEX = 2\n","UNK_INDEX = 3\n","\n","MARKER = [PAD, STD, END, UNK]\n","CHANGE_FILTER = re.compile(filters)"]},{"cell_type":"markdown","metadata":{"id":"xmRFuH2r6oNJ"},"source":["* 주소에서 데이터를 가져오는 `load_data()` 함수 선언\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"CmrmdXkePWYb","executionInfo":{"status":"ok","timestamp":1711881006663,"user_tz":-540,"elapsed":1346,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","def load_data(data_path):\n","  data_df = pd.read_csv(data_path, header = 0)\n","  question, answer = list(data_df['Q']), list(data_df['A'])\n","  train_input, eval_input, train_label, eval_label = train_test_split(question, answer,\n","                                                                      test_size = 0.33,\n","                                                                      random_state=111)\n","  return train_input, train_label, eval_input , eval_label"]},{"cell_type":"markdown","metadata":{"id":"vHuOJHPtPXqq"},"source":["* 처리에 필요한 단어 사전을 생성하는 `load_vocab()` 함수 선언"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"QtQL-AP06oSa","executionInfo":{"status":"ok","timestamp":1711881006664,"user_tz":-540,"elapsed":8,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def load_vocabulary(data_path):\n","  data_df = pd.read_csv(data_path, encoding='utf-8')\n","  question, answer = list(data_df['Q']), list(data_df['A'])\n","  if tokenize_as_morph:\n","    question = prepro_like_morphlized(question)\n","    answer = prepro_like_morphlized(answer)\n","\n","  data = []\n","  data.extend(question)\n","  data.extend(answer)\n","  words = data_tokenizer(data)\n","  words = list(set(words))\n","  words[:0] = MARKER\n","\n","  char2idx = {char:idx for idx, char in enumerate(words)}\n","  idx2char = {idx:char for idx, char in enumerate(words)}\n","  return char2idx, idx2char, len(char2idx)"]},{"cell_type":"markdown","metadata":{"id":"5wYtpjv76r5q"},"source":["* 문자열 데이터를 학습에 사용될 수 있도록 변현하는 `prepro_like_morphlized()` 함수 선언\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"-bQ3FOva6tg6","executionInfo":{"status":"ok","timestamp":1711881006664,"user_tz":-540,"elapsed":8,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["from konlpy.tag import Okt\n","\n","def prepro_like_morphlized(data):\n","  morph_analyzer = Okt()\n","  result_data = list()\n","  for seq in data :\n","    morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n","    result_data.append(morphlized_seq)\n","  return result_data"]},{"cell_type":"markdown","metadata":{"id":"vhsVp4pWPTR3"},"source":["* 단어 사전을 만들기 위해 단어들을 분리하는 `data_tokenizer()` 함수 선언"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"otLI_RUfPR_g","executionInfo":{"status":"ok","timestamp":1711881006664,"user_tz":-540,"elapsed":7,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def data_tokenizer(data):\n","  words = []\n","  for sentence in data:\n","    sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n","    for word in sentence.split():\n","      words.append(word)\n","    return [word for word in words if word]"]},{"cell_type":"markdown","metadata":{"id":"OkKPA-Mx6uaC"},"source":["* encoder의 입력을 구성하기 위한 함수 `enc_processing()` 선언\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"jK-yeSThPGsa","executionInfo":{"status":"ok","timestamp":1711881006664,"user_tz":-540,"elapsed":7,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["import numpy as np\n","\n","def enc_processing(value, dictionary):\n","  sequences_input_index = []\n","  sequences_length = []\n","\n","  if tokenize_as_morph:\n","    value = prepro_like_morphlized(value)\n","\n","  for sequence in value:\n","    sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","    sequence_index = []\n","    for word in sequence.split():\n","      if dictionary.get(word) is not None:\n","        sequence_index.extend([dictionary[word]])\n","      else:\n","        sequence_index.extend([dictionary[UNK]])\n","    if len(sequence_index) > max_len :\n","      sequence_index = sequence_index[:max_len]\n","    sequences_length.append(len(sequence_index))\n","    sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","    sequences_input_index.append(sequence_index)\n","  return np.asarray(sequences_input_index), sequences_length\n"]},{"cell_type":"markdown","metadata":{"id":"d4mM57_FPIg7"},"source":["* decoder의 입력을 구성하기 위한 함수 `dec_output_processing()` 선언"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"cX_NpcTq6vw6","executionInfo":{"status":"ok","timestamp":1711881006664,"user_tz":-540,"elapsed":6,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def dec_output_processing(value, dictionary):\n","    sequences_output_index = []\n","    sequences_length = []\n","\n","    if tokenize_as_morph:\n","        value = prepro_like_morphlized(value)\n","\n","    for sequence in value:\n","        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","        sequence_index = []\n","        sequence_index = [dictionary[STD]] + [dictionary[word] for word in sequence.split()]\n","        if len(sequence_index) > max_len:\n","            sequence_index = sequence_index[:max_len]\n","        sequences_length.append(len(sequence_index))\n","        sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","        sequences_output_index.append(sequence_index)\n","    return np.asarray(sequences_output_index), sequences_length"]},{"cell_type":"markdown","metadata":{"id":"otsTEt4FPLJX"},"source":["* decoder의 출력을 구성하기 위한 함수 `dec_target_processing()` 선언"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"eeP0PWHEPMma","executionInfo":{"status":"ok","timestamp":1711881006664,"user_tz":-540,"elapsed":6,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def dec_target_processing(value, dictionary):\n","  sequences_target_index = []\n","\n","  if tokenize_as_morph:\n","    value = prepro_like_morphlized(value)\n","\n","  for sequence in value:\n","    sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n","    sequence_index =[dictionary[word] for word in sequence.split()]\n","    if len(sequence_index) >= max_len :\n","      sequence_index = sequence_index[:max_len-1]+ [dictionary[END]]\n","    else:\n","      sequence_index += [dictionary[END]]\n","    sequence_index += (max_len - len(sequence_index)) * [dictionary[PAD]]\n","    sequences_target_index.append(sequence_index)\n","  return np.asarray(sequences_target_index)"]},{"cell_type":"markdown","metadata":{"id":"Tb9vVUng6xDq"},"source":["* 모델에 데이터를 효율적으로 투입하도록 `train_input_fn()`, `eval_input_fn()` 함수 선언\n","* `rearrange()`는 dataset 객체가 데이터를 어떻게 변형시킬지 정의해둔 함수\n","* dataset.map은 rearrange 함수를 기반으로 데이터를 변형\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"uAlKV4xF62Uf","executionInfo":{"status":"ok","timestamp":1711881006665,"user_tz":-540,"elapsed":6,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def train_input_fn(train_input_enc, train_output_enc, train_target_dec, batch_size):\n","  dataset = tf.compat.v1.data.Dataset.tensor_slices((train_input_enc, train_output_enc, train_target_dec))\n","  dataset = dataset.shuffle(buffer_size = len(train_input_enc))\n","  dataset = dataset.batch(batch_size)\n","  dataset = dataset.map(rearrange)\n","  dataset = dataset.repeat()\n","  iterator = dataset.make_one_shot_iterator()\n","  return iterator.get_next()\n","\n","def eval_input_fn(eval_input_enc, eval_output_enc, eval_target_dec, batch_size):\n","  dataset = tf.compat.v1.data.Dataset.tensor_slices((eval_input_enc, eval_output_enc, eval_target_dec))\n","  dataset = dataset.shuffle(buffer_size = len(eval_input_enc))\n","  dataset = dataset.batch(batch_size)\n","  dataset = dataset.map(rearrange)\n","  dataset = dataset.repeat(1)\n","  iterator = dataset.make_one_shot_iterator()\n","  return iterator.get_next()\n","\n","def rearrange(input, output, target):\n","  features = {'input': input, 'output': output}\n","  return features, target"]},{"cell_type":"markdown","metadata":{"id":"is-GhUDN62xC"},"source":["* 모델의 예측은 배열로 생성되기 때문에 이를 확인하기 위해선 문자열로 변환이 필요\n","* 예측을 문자열로 변환해주는 `pred2string()` 함수 선언\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"jCfwWXhb64Cc","executionInfo":{"status":"ok","timestamp":1711881006665,"user_tz":-540,"elapsed":6,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def pred2string(value, dictionary):\n","  sentence_string = []\n","  is_finished = False\n","\n","  for v in value:\n","    sentence_string = [dictionary[index] for index in v['indexs']]\n","\n","  answer = \"\"\n","  for word in sentence_string:\n","    if word == END:\n","      is_finished = True\n","      break\n","\n","    if word != PAD and word != END:\n","      answer += word\n","      answer += \" \"\n","\n","  return answer, is_finished"]},{"cell_type":"markdown","metadata":{"id":"hwp9Nnwz7UoG"},"source":["* 챗봇 데이터 URL: https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData%20.csv\n","* 데이터 주소에서 데이터를 읽어들여 단어 사전과 사용 데이터 구성"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"-T536MdU7Taq","executionInfo":{"status":"ok","timestamp":1711881141747,"user_tz":-540,"elapsed":135087,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["import pandas as pd\n","tokenize_as_morph = True\n","\n","data_path = ' https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv'\n","\n","char2idx, idx2char, len_vocab = load_vocabulary(data_path)\n","train_input, train_label, eval_input, eval_label = load_data(data_path)"]},{"cell_type":"markdown","metadata":{"id":"7cVd7AOKinqn"},"source":["### 모델 구성"]},{"cell_type":"markdown","metadata":{"id":"hqLJ0a6r49yi"},"source":["* 앞서 작성한 트랜스포머 모델을 결합해 학습에 사용할 모델을 구성함"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"CNeeXoZginvj","executionInfo":{"status":"ok","timestamp":1711881141747,"user_tz":-540,"elapsed":15,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def model(features, labels, mode, params):\n","  TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n","  EVAL = mode == tf.estimator.ModeKeys.EVAL\n","  PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n","\n","  position_encede = positional_encoding(params['embedding_size'], params['max_len'])\n","  if params['xavier_initializer']:\n","    embedding_initializer = 'glorot_normal'\n","  else:\n","    embedding_initializer = 'uniform'\n","\n","  embedding = tf.keras.layers.Embedding(params['len_vocab'],\n","                                        params['embedding_size'],\n","                                        embeddings_initializer = embedding_initializer)\n","\n","  x_embedded_matrix = embedding(features['input']) + position_encode\n","  y_embedded_matrix = embedding(features['output'] + position_encode)\n","\n","  encoder_outputs = encoder(x_embedded_matrix, params['model_hidden_size'], params['ffn_hidden_size'],\n","                            params['attention_head_size'], params['layer_size'])\n","  decoder_outputs = decoder(y_embedded_matrix, encoder_outputs, params['model_hidden_size'],\n","                            params['ffn_hidden_size'], params['attention_head_size'], params['layer_size'])\n","\n","  logits = tf.keras.layers.Dense(params['len_vocab'])(decoder_outputs)\n","  predict = tf.argmax(logits,2)\n","\n","  if PREDICT:\n","    predictions = {'indexs':predict,\n","                   'logits': logits}\n","    return tf.estimator.EstimatorSpec(mode, predictions = predictions)\n","\n","  labels_ = tf.one_hot(labels, params['len_vocab'])\n","  loss = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n","  accuracy = tf.compat.v1.metrics.accuracy(labels=labels, predictions=predict)\n","\n","  metrics = {'accuracy':accuracy}\n","  tf.summary.scalar('accuracy', accuracy[1])\n","\n","  if EVAL:\n","    return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n","  assert TRAIN\n","\n","  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate = params['learning_rate'])\n","  train_op = optimizer.minimize(loss, global_step = tf.compat.v1.train.get_global_step())\n","  return tf.estimator.EstimatorSpec(mode, loss=loss, train_op = train_op)"]},{"cell_type":"markdown","metadata":{"id":"H7PrLEWE1JCs"},"source":["### 모델 학습"]},{"cell_type":"markdown","metadata":{"id":"Gy_Opm_A7DKC"},"source":["*   필요한 각종 인자들을 설정\n","*   인자에 따라 학습 결과가 달라질 수 있기 때문에 세심한 조정이 필요\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"CKGYuqmH6_kj","executionInfo":{"status":"ok","timestamp":1711881141747,"user_tz":-540,"elapsed":4,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["max_len = 25\n","epoch = 5000\n","batch_size = 256\n","embedding_size =100\n","model_hidden_size =100\n","ffn_hidden_size = 100\n","attention_head_size =100\n","lr = 0.001\n","layer_size = 3\n","xavier_initializer = True"]},{"cell_type":"markdown","metadata":{"id":"aaXalEy57ODq"},"source":["*   앞서 선언한 processing 함수로 데이터를 모델에 투입할 수 있도록 가공\n","*   평가 데이터에도 동일하게 가공"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":370},"executionInfo":{"elapsed":60481,"status":"error","timestamp":1711881202225,"user":{"displayName":"이다빈","userId":"05137797752462423239"},"user_tz":-540},"id":"NWlgWWIq1KSh","outputId":"52fc678b-5cf7-45e9-afc4-63cbb5ff4248"},"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'로또'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-1c60acef2537>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_input_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input_enc_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_output_dec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output_dec_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_output_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_target_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_target_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0meval_input_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_input_enc_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar2idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-301e26c7d317>\u001b[0m in \u001b[0;36mdec_output_processing\u001b[0;34m(value, dictionary)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHANGE_FILTER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msequence_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msequence_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0msequence_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-301e26c7d317>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHANGE_FILTER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0msequence_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0msequence_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0msequence_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: '로또'"]}],"source":["train_input_enc, train_input_enc_length = enc_processing(train_input, char2idx)\n","train_output_dec, train_output_dec_length = dec_output_processing(train_input, char2idx)\n","train_target_dec = dec_target_processing(train_label, char2idx)\n","\n","eval_input_enc, eval_input_enc_length = enc_processing(eval_input, char2idx)\n","eval_output_dec, eval_output_dec_length = dec_output_processing(eval_input, char2idx)\n","eval_target_dec = dec_target_processing(eval_label, char2idx)"]},{"cell_type":"markdown","metadata":{"id":"qZGgZzWs7Mr7"},"source":["* 앞서 선언한 함수를 통해 모델을 선언하고 학습\n","* `tf.estimator`를 사용해 간편하게 학습 모듈 구성\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9vjc3Ck7F4J","executionInfo":{"status":"aborted","timestamp":1711881202226,"user_tz":-540,"elapsed":14,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["transformer = tf.estimator.Estimator(\n","    model_fn = model,\n","    params = {'embedding_size': embedding_size,\n","              'model_hidden_size': model_hidden_size,\n","              'ffn_hidden_size': ffn_hidden_size,\n","              'attention_head_size': attention_head_size,\n","              'learning_rate': lr,\n","              'len_vocab': len_vocab,\n","              'layer_size': layer_size,\n","              'max_len': max_len,\n","              'xavier_initializer': xavier_initializer}\n",")"]},{"cell_type":"markdown","metadata":{"id":"wl_pwUiw7INZ"},"source":["* 학습한 모델을 사용해 챗봇을 사용\n","* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n","* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"COO-0PcS7Hy5","executionInfo":{"status":"aborted","timestamp":1711881202227,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["transformer.train(input_fn = lambda: train_input_fn(train_input_enc, train_output_dec, train_target_dec, batch_size), steps = epoch)\n","eval_result = transformer.evaluate(input_fn = lambda: eval_input_fn(train_input_enc, eval_output_dec, eval_target_dec, batch_size))\n","\n","print(\"{accuracy: 0.3f}\".format(**eval_result))"]},{"cell_type":"markdown","metadata":{"id":"MNcrVf2z1LSM"},"source":["### 예측"]},{"cell_type":"markdown","metadata":{"id":"R5lY9DrW8eSK"},"source":["* 학습한 모델을 사용해 챗봇을 사용\n","* 예측 결과를 문자열로 변환할 때는 앞서 선언한 `pred2string()` 함수를 이용\n","* 입력에 대한 응답이 생성되는 것을 확인할 수 있음\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9IQaBx4Qw8J","executionInfo":{"status":"aborted","timestamp":1711881202227,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["def chatbot(sentence):\n","    pred_input_enc, pred_input_enc_length = enc_processing([sentence], char2idx)\n","    pred_output_dec, pred_output_dec_length = dec_output_processing([\"\"], char2idx)\n","    pred_target_dec = dec_target_processing([\"\"], char2idx)\n","\n","    for i in range(max_len):\n","        if i > 0:\n","            pred_output_dec, pred_output_dec_length = dec_output_processing([answer], char2idx)\n","            pred_target_dec = dec_target_processing([answer], char2idx)\n","\n","        predictions = transformer.predict(input_fn = lambda: eval_input_fn(pred_input_enc, pred_output_dec, pred_target_dec, 1))\n","\n","        answer, finished = pred2string(predictions, idx2char)\n","\n","        if finished:\n","            break\n","\n","    return answer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjHZKvJ31MAU","executionInfo":{"status":"aborted","timestamp":1711881202227,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["chatbot(\"안녕?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mjRZwyLQ_gP","executionInfo":{"status":"aborted","timestamp":1711881202228,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["chatbot(\"너 누구냐?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7AJCsXRTqJx","executionInfo":{"status":"aborted","timestamp":1711881202228,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["chatbot(\"뭐 먹었어?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_M8mfoUfeAWQ","executionInfo":{"status":"aborted","timestamp":1711881202229,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["chatbot(\"놀고 싶다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5mrdGRaem6v","executionInfo":{"status":"aborted","timestamp":1711881202229,"user_tz":-540,"elapsed":13,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"outputs":[],"source":["chatbot(\"이제 그만 잘래\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1qBC_BPdmQgTWSd6n1B4d-pNAaATy_xVC","timestamp":1711427946090}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}