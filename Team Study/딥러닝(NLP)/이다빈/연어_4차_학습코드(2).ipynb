{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+AK5thFm9DdeBs2RQXokt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### GPT-2 학습해보기"],"metadata":{"id":"8EzdUJK7o1Fo"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E00Y9-YXhB28","executionInfo":{"status":"ok","timestamp":1712494794278,"user_tz":-540,"elapsed":28175,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"dcc05eb2-e3bd-47a8-8bf3-ec41921cf360"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.30\n","  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (3.13.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30) (2024.2.2)\n","Installing collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.15.2\n","    Uninstalling tokenizers-0.15.2:\n","      Successfully uninstalled tokenizers-0.15.2\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.38.2\n","    Uninstalling transformers-4.38.2:\n","      Successfully uninstalled transformers-4.38.2\n","Successfully installed tokenizers-0.13.3 transformers-4.30.0\n"]}],"source":["!pip install transformers==4.30"]},{"cell_type":"code","source":["import torch\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NWv2zMtpXLP","executionInfo":{"status":"ok","timestamp":1712494900129,"user_tz":-540,"elapsed":5309,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"7f145c0d-95d8-421f-8f50-a95ca7ca8249"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"paeA9jC3qj4b","executionInfo":{"status":"ok","timestamp":1712495163182,"user_tz":-540,"elapsed":7692,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"336abf69-d635-46aa-ab10-28ac5ed7d766"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"]}]},{"cell_type":"code","source":["!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1V4rTx4yaAg0x1NY1MpNRY2Dp1nKeyOQ7\" -o wiki_20190620_small.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VaYAnlPkql-Y","executionInfo":{"status":"ok","timestamp":1712495175902,"user_tz":-540,"elapsed":5190,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"f42a6143-f7c7-4c74-9211-ccaf0a299a51"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["awk: cannot open ./cookie (No such file or directory)\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100 75.1M  100 75.1M    0     0  11.7M      0  0:00:06  0:00:06 --:--:-- 18.3M\n"]}]},{"cell_type":"code","source":["path = '/content/wiki_20190620_small.txt'"],"metadata":{"id":"uXFVdnSupXI3","executionInfo":{"status":"ok","timestamp":1712495177781,"user_tz":-540,"elapsed":374,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["지금까지는 BertWordPieceTokenizer를 사용해왔다면, <br>\n","이번에는 SentencePiceBPETokenizer를 사용해 모델을 학습해보겠습니다. <br>"],"metadata":{"id":"DeYuGY_oqD8n"}},{"cell_type":"code","source":["# 내부적인 알고리즘은 BPE 로 동일하고 tokenizer 의 경우에는 띄어쓰기 부분에 ‘_’ 기호를 추가하는 과정만 다르다고 생각하면 됨\n","from tokenizers import SentencePieceBPETokenizer\n","from tokenizers.normalizers import BertNormalizer\n","\n","tokenizer = SentencePieceBPETokenizer()\n","\n","tokenizer._tokenizer.normalizer = BertNormalizer(clean_text=True,\n","handle_chinese_chars=False,\n","lowercase=False)\n","\n","# 마찬가지로 tokenizer 학습\n","# 학습데이터와 vocab_size 를 지정\n","tokenizer.train(\n","    path,\n","    vocab_size=10000,\n","    special_tokens=[\n","        \"<s>\",\n","        \"<pad>\",\n","        \"</s>\",\n","        \"<unk>\",\n","    ],\n",")"],"metadata":{"id":"8xqbflj-pXGm","executionInfo":{"status":"ok","timestamp":1712495209727,"user_tz":-540,"elapsed":30475,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["BERT 와 다른점이 하나 있는데 BERT 는 학습할 때 앞에 [CLS] 토큰과 뒤에 [SEP] 토큰을 부착하게 되어있었음\n","\n","그렇기 때문에 special token 에 [PAD], [CLS] [SEP], [UNK] 들이 의무적으로 들어갔었음\n","\n","하지만 GPT2 가 학습하는 과정을 상상해보면 앞에 토큰이 나왔을 때 계속해서 다음 토큰을 예측하는 방식으로 이루어지고 있음\n","\n","그래서 [CLS] 토큰이나 [SEP] 토큰이 필요하지 않음\n","\n","하지만 이 경우에 fasttext 도 상상해보면 음절의 시작과 긑을 표시해주는 “<”, “>” 기호 넣었음\n","\n","역시 GPT2 도 문장의 끝과 시작을 표시해주는 special token 을 부착해줄 수 있음\n","\n","그래서 나중에 생성할 때 문장단위로 생성하도록 control 할 수 있음\n","\n","special token 에 문장의 시작을 의미하는 < s > 와 문장의 끝을 의미하는 < /s> 를 추가해줄 수 있음\n","\n","tokenizer 를 학습했으니 test 를 해보자."],"metadata":{"id":"EAxdzjIurFTc"}},{"cell_type":"code","source":["print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\"))\n","print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n","print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n","print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))\n","\n","# 문장에서 띄어쓰기가 들어가는 부분 즉, 어절의 시작부분에는 ‘_’ 기호가 부착이됨\n","# SentencePiece를 사용하면, 나중에 decoding 과정에서 '_' 만 ' '로 replace해주면 띄어쓰기 복원이 가능해집니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QXe8U9heqWTj","executionInfo":{"status":"ok","timestamp":1712495413861,"user_tz":-540,"elapsed":412,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"255e7fc3-afed-4b5b-f2b1-a35a45b51b9d"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n","[6284, 693, 1246, 6180, 698, 6057, 699, 262, 13]\n","['▁이순신', '은', '▁조선', '▁중기', '의', '▁무신', '이', '다', '.']\n","이순신은 조선 중기의 무신이다.\n"]}]},{"cell_type":"code","source":["# tokniezr 를 저장할 땐 아래 명령어를 사용\n","tokenizer.save_model(\".\")\n","# 나만의 tokenizer 가 만들어졌음"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8HFDDmlpXEQ","executionInfo":{"status":"ok","timestamp":1712495485334,"user_tz":-540,"elapsed":344,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"c2982652-f94a-4bb5-8a26-780ecd5bf9fa"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['./vocab.json', './merges.txt']"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["tokenizer = SentencePieceBPETokenizer.from_file(vocab_filename=\"vocab.json\", merges_filename=\"merges.txt\")\n","# from_file() 함수를 사용하면 내가 학습한 모델을 그대로 불러와서 tokenizer 로서 사용할 수 있음"],"metadata":{"id":"T-RhnRRerxle","executionInfo":{"status":"ok","timestamp":1712495520794,"user_tz":-540,"elapsed":9,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\"))\n","print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").ids)\n","print(tokenizer.encode(\"이순신은 조선 중기의 무신이다.\").tokens)\n","print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n","print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))\n","\n","# 불러와서 test 해보면 동일한 결과를 확인가능\n","# 우리가 원하는 것은 문장의 시작과 끝에 <s>, </s> 를 부착함으로써 문장을 구분해내는 것을 원하고 있음\n","\n","# 그런데 이렇게 다 분리되서 나옴\n","# 이유는 special token 으로 지정이 되어있지 않음"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnJ4Jb6yrxiI","executionInfo":{"status":"ok","timestamp":1712495549575,"user_tz":-540,"elapsed":11,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"80b14896-472e-4563-ec5d-dfa2fd897e83"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n","[6284, 693, 1246, 6180, 698, 6057, 699, 262, 13]\n","['▁이순신', '은', '▁조선', '▁중기', '의', '▁무신', '이', '다', '.']\n","['▁<', 's', '>', '이', '순', '신', '은', '▁조선', '▁중기', '의', '▁무신', '이', '다', '.', '<', '/', 's', '>']\n","<s>이순신은 조선 중기의 무신이다.</s>\n"]}]},{"cell_type":"code","source":["# 의도적으로 tokenizer 에 add_special_tokens 를 통해 명시적으로 알려줘야 함\n","\n","tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<unk>\", \"<pad>\", \"<shkim>\"])\n","tokenizer.pad_token_id = tokenizer.token_to_id(\"<pad>\")\n","tokenizer.unk_token_id = tokenizer.token_to_id(\"<unk>\")\n","tokenizer.bos_token_id = tokenizer.token_to_id(\"<bos>\")\n","tokenizer.eos_token_id = tokenizer.token_to_id(\"<eos>\")\n","\n","print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids)\n","print(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").tokens)\n","print(tokenizer.decode(tokenizer.encode(\"<s>이순신은 조선 중기의 무신이다.</s>\").ids, skip_special_tokens=True))\n","\n","# 이렇게 등록하고 나면 문장내에 스페셜 토큰을 부착하게 되더라도 올바르게 tokenizing 된 것은 확인할 수 있음"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6apyzLo7rxfe","executionInfo":{"status":"ok","timestamp":1712495631602,"user_tz":-540,"elapsed":11,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"c43bd04f-3925-4003-ed09-3e4c80ac6d03"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 6284, 693, 1246, 6180, 698, 6057, 699, 262, 13, 2]\n","['<s>', '▁이순신', '은', '▁조선', '▁중기', '의', '▁무신', '이', '다', '.', '</s>']\n","이순신은 조선 중기의 무신이다.\n"]}]},{"cell_type":"code","source":["# 본격적으로 GPT2 를 학습\n","from transformers import GPT2Config, GPT2LMHeadModel\n","\n","config = GPT2Config(\n","  vocab_size=tokenizer.get_vocab_size(),\n","  bos_token_id=tokenizer.token_to_id(\"<s>\"),\n","  eos_token_id=tokenizer.token_to_id(\"</s>\"),\n",")\n","# 모델생성\n","model = GPT2LMHeadModel(config)\n","\n","# transformers 에서 BERT 와 마찬가지로 GPT2 에 대해서도 학습을 제공해줌\n","# GPT2 에 대한 껍데기 모델을 만들어야 함\n","# 항상 vocab_size 를 우리가 만든 tokenizer 의 vocab_size 로 명확하게 지정해줘야 함\n","# bos_token(문장의 시작을 알려주는 토큰) 과 eos_token(문장의 끝을 알려주는 토큰)을 어떤 것으로 정의했는지 명확하게 저장할 수 있음\n","# 다양한 옵션들이 많아서 layer size 를 줄일 수도 있고 내부의 embedding size 도 줄일 수 있고 다양한 설정을 할 수 있음"],"metadata":{"id":"weQgsvm3rxdG","executionInfo":{"status":"ok","timestamp":1712495734925,"user_tz":-540,"elapsed":4942,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["model.num_parameters() # 파라미터 개수도 알 수 있음"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5lYbAF-2secX","executionInfo":{"status":"ok","timestamp":1712495782320,"user_tz":-540,"elapsed":9,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"0283497c-d636-4d89-a793-0fc6120e99fb"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["93523200"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# GPT2 에 넣어줄 데이터셋도 만들어야 함\n","import json\n","import os\n","import pickle\n","import random\n","import time\n","import warnings\n","from typing import Dict, List, Optional\n","\n","import torch\n","from torch.utils.data.dataset import Dataset\n","\n","from filelock import FileLock\n","\n","from transformers.tokenization_utils import PreTrainedTokenizer\n","from transformers.utils import logging\n","\n","logger = logging.get_logger(__name__)\n","\n","class TextDataset(Dataset):\n","    \"\"\"\n","    This will be superseded by a framework-agnostic approach soon.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        tokenizer: PreTrainedTokenizer,\n","        file_path: str,\n","        block_size: int,\n","        overwrite_cache=False,\n","        cache_dir: Optional[str] = None,\n","    ):\n","        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n","\n","        block_size = block_size - tokenizer.num_special_tokens_to_add(is_pair=False)\n","\n","        directory, filename = os.path.split(file_path)\n","        cached_features_file = os.path.join(\n","            cache_dir if cache_dir is not None else directory,\n","            \"cached_lm_{}_{}_{}\".format(\n","                tokenizer.__class__.__name__,\n","                str(block_size),\n","                filename,\n","            ),\n","        )\n","\n","        lock_path = cached_features_file + \".lock\"\n","        with FileLock(lock_path):\n","\n","            if os.path.exists(cached_features_file) and not overwrite_cache:\n","                start = time.time()\n","                with open(cached_features_file, \"rb\") as handle:\n","                    self.examples = pickle.load(handle)\n","                logger.info(\n","                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n","                )\n","\n","            else:\n","                logger.info(f\"Creating features from dataset file at {directory}\")\n","                # 여기서부터 본격적으로 데이터셋을 만들기 시작\n","                self.examples = []\n","                text = \"\"\n","                with open(file_path, encoding=\"utf-8\") as f: #  file_path(wiki small 데이터)\n","                    lines = f.readlines()\n","                    for line in lines:\n","                        line = line.strip()\n","                        line = \"<s>\"+line+\"</s>\" # 학습 데이터 앞 뒤에 문장 구분 기호를 추가\n","                        text += line    # 'text' 객체에 모든 학습 데이터를 다 합쳐버림\n","                tokenized_text = tokenizer.encode(text).ids\n","\n","                # 모델의 최대 sequence length만큼 데이터를 잘라서 저장\n","                for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n","                    self.examples.append(\n","                        tokenized_text[i : i + block_size]\n","                    )\n","\n","                start = time.time()\n","                with open(cached_features_file, \"wb\") as handle:\n","                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","                logger.info(\n","                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n","                )\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, i) -> torch.Tensor:\n","        return torch.tensor(self.examples[i], dtype=torch.long)"],"metadata":{"id":"__8sKoh2seZs","executionInfo":{"status":"ok","timestamp":1712496216822,"user_tz":-540,"elapsed":446,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["GPT2 모델은 BERT 학습보다 훨씬 간단함\n","\n","mask 의 대한 개념도 없었고 Next Sentence Prediction 도 필요하지 않음\n","\n","그냥 우리가 학습해야하는 sentence 를 통채로 주면 GPT 모델이 이 토큰 다음에 나올 수 있는 토큰은 이거다 다음은 이거다 라는 것을 순차적으로 자동으로 학습하게됨\n","\n","그래서 sentence 만 만들어줘서 GPT 모델에 넘겨주면 됨\n","\n","캐싱 관련된 부분을 지나서 file_path(wiki small 데이터) 를 한줄씩 읽어서 strip() 해주고 의도적으로 line 앞뒤에 < s >, < /s > 를 넣어주게 됨 그리고 text 라는 하나의 변수에 모든 학습데이터를 통채로 넣게 됨\n","\n","text 안에 기존의 wiki 데이터와 동일하지만 모든 문장마다 < s >, < /s> 이 부착된 형태로 text 가 만들어짐\n","\n","그 만들어진 것을 tokenizing 하면 tokenizer 에 따라서 vocab_id 로 변환된 데이터가 tokenized_text 로 입력으로 들어가게 됨\n","\n","그러면 내가 가진 모델의 sequence length 가 있을 건데 BERT 는 512 토큰이 있었고 GPT2 도 그런 block_size 를 가질텐데 tokenized_text 데이터는 전체 wiki 의 데이터를 다가지고 있으니 이거를 block_size 만큼으로 쪼개게 됨\n","\n","toknized_text[i : i + bolck_size] 로 쪼개서 exmaples 에 넣게 됨\n","\n","다 넣게되면 우리가 원하는 학습데이터가 완성됨"],"metadata":{"id":"QmXCe3C5tNgx"}},{"cell_type":"code","source":["dataset = TextDataset(\n","    tokenizer=tokenizer,\n","    file_path=path,\n","    block_size=128,\n",")\n","from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(    # GPT는 생성모델이기 때문에 [MASK] 가 필요 없다\n","    tokenizer=tokenizer, mlm=False, # mask를 의미하는 것을 False로\n",")\n","# TextDataset 에 tokenizer, file_path, block_size 넣고 만들어 줌\n","# 실습을 위해 block_size 를 128 로 줬지만 원래 사이즈는 1024"],"metadata":{"id":"waasDxnyseXG","executionInfo":{"status":"ok","timestamp":1712496330725,"user_tz":-540,"elapsed":110748,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["이렇게 데이터셋인 밥이 만들어지면 떠먹여주는 데이터로더가 필요함\n","\n","transformers 에서 DataCollatorForLanguageModeling 이라는 함수를 제공해주고 있음\n","\n","이전에는 data_collator 를 mask 를 지원해주는 기능을 넣었었는데 이번에는 mask 가 필요없음\n","\n","그래서 LanguageModeling 을 위한 함수를 불러왔음"],"metadata":{"id":"PfeWPIjuvI1l"}},{"cell_type":"code","source":["# 이 함수를 call 할 때 mlm 은 mask 를 의미하는 건데 GPT2 는 mask 가 필요없으므로 False 로 두고 데이터셋을 만들면\n","print(dataset[0])\n","# 학습을 위한 데이터셋이 만들어지게 됨"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W3p3xP0xseUW","executionInfo":{"status":"ok","timestamp":1712496407490,"user_tz":-540,"elapsed":9,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"7164fee5-1d5e-4685-f837-8b1179f3a02f"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([   0, 4291, 1911, 1076, 1021,  456, 1076, 1245,  874, 1050, 5751, 1009,\n","        2694, 1808, 1197, 3942, 1182, 1390, 1340, 1005, 1004, 1007,    2,    0,\n","        1021,  456, 1245,  874, 1009, 5000,  746, 1489,  874, 6691, 6076,  701,\n","         579, 2442, 1025, 2295, 1004, 1007,    2,    0, 5000, 8593, 1758, 1016,\n","        2129, 1011, 1039, 1004, 1007,    2,    0, 1029, 1103, 2539, 1006, 2546,\n","        8277, 1138, 7604, 1138, 5319,  943, 1008, 1271, 3629, 1020, 1036, 1011,\n","        1039, 1004, 1007,    2,    0, 4825, 1024, 1197, 2539, 7387, 1022, 1178,\n","         916, 1011, 1039, 1017, 1169, 2120,  845, 1138, 9501, 1071, 1012, 1014,\n","           3, 1162, 1013, 2407, 1012, 1615, 1030, 1004, 1007,    2,    0, 1029,\n","        1008, 4572, 1005, 1076, 2120,  845, 5414, 1076, 1022, 1441, 1374, 1004,\n","        1007,    2,    0, 4288, 1024, 5000, 1050, 4229])\n"]}]},{"cell_type":"code","source":["# 동일하게 TrainArgument 에 우리가 원하는 argument 를 채워주게 됨\n","\n","from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir='model_output',\n","    overwrite_output_dir=True,\n","    num_train_epochs=50,\n","    per_device_train_batch_size=64, # 512:32  # 128:64\n","    save_steps=1000,\n","    save_total_limit=2,\n","    logging_steps=100\n","\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":551},"id":"X_U0T_vxrxa2","executionInfo":{"status":"error","timestamp":1712496459419,"user_tz":-540,"elapsed":595,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"66d2d680-1cd3-493c-88bf-5d5dc813742f"},"execution_count":23,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-d34a4846761d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_output'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moverwrite_output_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_met...\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mget_xla_device_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"GPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16_full_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36mdevice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1762\u001b[0m         \"\"\"\n\u001b[1;32m   1763\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1764\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m_setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0.20.1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   1673\u001b[0m                     \u001b[0;34m\"Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m                 )\n","\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["trainer.train()\n","# BERT 와 마찬가지로 굉장히 쉽게 GPT2 도 학습할 수 있음"],"metadata":{"id":"81y5BtCjrxYg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.save_model()"],"metadata":{"id":"zkxDAiCEvtXV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습을 했으니 test\n","USE_GPU = 1\n","# Device configuration\n","device = torch.device('cuda' if (torch.cuda.is_available() and USE_GPU) else 'cpu')"],"metadata":{"id":"YETuZpMKrxUa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","torch.manual_seed(42)\n","\n","input_ids = torch.tensor(tokenizer.encode(\"<s>이순신\", add_special_tokens=True).ids).unsqueeze(0).to('cuda')\n","\n","output_sequences = model.generate(input_ids=input_ids, do_sample=True, max_length=100, num_return_sequences=3)\n","for generated_sequence in output_sequences:\n","    generated_sequence = generated_sequence.tolist()\n","    print(\"GENERATED SEQUENCE : {0}\".format(tokenizer.decode(generated_sequence, skip_special_tokens=True)))"],"metadata":{"id":"uQ9DLz7rvrdf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["테스트를 할 때는 GPT2 모델 같은 경우에 generate 라는 특별한 함수를 제공해주고 있음\n","\n","다음 실습에서는 generate 함수에 대한 분석을 보여주겠음\n","\n","간단히 설명하자면 입력값이 들어가고 GPT2 가 단어를 생성하기에 앞서서 앞에 무슨 단어를 시작으로 할 것이냐를 input_ids 로 넣게 됨\n","\n","input_ids 이기 때문에 당연히 tokenize 된 vocab_id 가 들어가게 됨\n","\n","do_sample=True 면 sample 을 몇개 반환하라는 의미"],"metadata":{"id":"uga8Z3C6v3mE"}}]}