{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1cyRyTv3BZHahrjmbvnNW0tZkFDT0lM-G","timestamp":1711427915681}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zBjBNQX8kQfh"},"source":["# GPT(Generative Pre-trained Transformer) 2\n","\n","* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"]},{"cell_type":"markdown","metadata":{"id":"gKeqNH_dkTmT"},"source":["* OpenAI에서 GPT 모델 제안\n","* 매우 큰 자연어 처리 데이터를 활용해 비지도 학습으로 사전 학습 후 학습된 가중치를 활용해 파인 튜닝\n","* BERT와 마찬가지로 트랜스포머 모델이지만, BERT는 트랜스포머의 인코더 구조만 사용하고, GPT는 트랜스포머의 디코더 구조(순방향 어텐션)만 사용\n","\n","* GPT2는 GPT1에서 개선되어 레이어 정규화가 부분 블록의 입력쪽에서 사용되고, 셀프 어텐션 이후에 레이어 정규화 적용\n","* GPT2는 GPT1에 비교해 크기가 매우 커진 향상된 모델 사용"]},{"cell_type":"markdown","metadata":{"id":"sDCr0YqjbfLJ"},"source":["## 라이브러리"]},{"cell_type":"code","metadata":{"id":"_ixYBCR8bguE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711964718583,"user_tz":-540,"elapsed":60857,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"52c2c764-ea05-4108-ac5d-14a2f311db4c"},"source":["!pip install transformers==2.11.0\n","!pip install tensorflow==2.2.0\n","!pip install sentencepiece==0.1.85\n","!pip install gluonnlp==0.9.1\n","!pip install mxnet==1.6.0"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==2.11.0\n","  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m674.8/674.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (1.25.2)\n","Collecting tokenizers==0.7.0 (from transformers==2.11.0)\n","  Downloading tokenizers-0.7.0.tar.gz (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (24.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (3.13.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2.31.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (4.66.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2023.12.25)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (0.1.99)\n","Collecting sacremoses (from transformers==2.11.0)\n","  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2024.2.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (1.3.2)\n","Building wheels for collected packages: tokenizers\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0mFailed to build tokenizers\n","\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.2.0\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement sentencepiece==0.1.85 (from versions: 0.0.0, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.9, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.83, 0.1.86, 0.1.91, 0.1.92, 0.1.94, 0.1.95, 0.1.96, 0.1.97, 0.1.98, 0.1.99, 0.2.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for sentencepiece==0.1.85\u001b[0m\u001b[31m\n","\u001b[0mCollecting gluonnlp==0.9.1\n","  Downloading gluonnlp-0.9.1.tar.gz (252 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (1.25.2)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (3.0.9)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.9.1) (24.0)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.9.1-cp310-cp310-linux_x86_64.whl size=564583 sha256=3ec3c75ae02121417cff9da2305a4a45280e0441f19836914b8c943525405e51\n","  Stored in directory: /root/.cache/pip/wheels/fc/5b/9c/3295bb07f7c5544a96303a48988707816f44a536e8e1413922\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.9.1\n","Collecting mxnet==1.6.0\n","  Downloading mxnet-1.6.0-py2.py3-none-any.whl (68.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.6.0) (1.25.2)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet==1.6.0) (2.31.0)\n","Collecting graphviz<0.9.0,>=0.8.1 (from mxnet==1.6.0)\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet==1.6.0) (2024.2.2)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.3\n","    Uninstalling graphviz-0.20.3:\n","      Successfully uninstalled graphviz-0.20.3\n","Successfully installed graphviz-0.8.4 mxnet-1.6.0\n"]}]},{"cell_type":"code","source":["!pip3 install mxnet-mkl==1.6.0 numpy==1.23.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":499},"id":"trojStHGzaxu","executionInfo":{"status":"ok","timestamp":1711964747012,"user_tz":-540,"elapsed":28440,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"63ccff51-ee50-4cbb-e774-483af352967a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mxnet-mkl==1.6.0\n","  Downloading mxnet_mkl-1.6.0-py2.py3-none-manylinux1_x86_64.whl (76.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting numpy==1.23.1\n","  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (2.31.0)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet-mkl==1.6.0) (0.8.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet-mkl==1.6.0) (2024.2.2)\n","Installing collected packages: numpy, mxnet-mkl\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n","tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed mxnet-mkl-1.6.0 numpy-1.23.1\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"302b7c4fbf69419db010b0ee963c94f7"}},"metadata":{}}]},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import AutoTokenizer\n","from transformers import TFGPT2LMHeadModel"],"metadata":{"id":"Krli9uBJzhj9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VPhczTnFjsG4"},"source":["## 데이터 다운로드\n","\n","* https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt"]},{"cell_type":"code","metadata":{"id":"hyCKy2LtjsG7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711964747608,"user_tz":-540,"elapsed":309,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"38278c1d-bc24-455e-a96a-df552c32cf0d"},"source":["!mkdir -p gpt2\n","!wget https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt \\\n","      -O gpt2/finetune_data.txt"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-04-01 09:45:46--  https://raw.githubusercontent.com/NLP-kr/tensorflow-ml-nlp-tf2/master/7.PRETRAIN_METHOD/data_in/KOR/finetune_data.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 24570 (24K) [text/plain]\n","Saving to: ‘gpt2/finetune_data.txt’\n","\n","\rgpt2/finetune_data.   0%[                    ]       0  --.-KB/s               \rgpt2/finetune_data. 100%[===================>]  23.99K  --.-KB/s    in 0.001s  \n","\n","2024-04-01 09:45:47 (15.7 MB/s) - ‘gpt2/finetune_data.txt’ saved [24570/24570]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"-pGgee8pjsHB","executionInfo":{"status":"ok","timestamp":1711964774367,"user_tz":-540,"elapsed":19612,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["import os\n","import numpy as np\n","import gluonnlp as nlp\n","from gluonnlp.data import SentencepieceTokenizer\n","from nltk.tokenize import sent_tokenize\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from transformers import TFGPT2LMHeadModel"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ROOajn6VIzgy"},"source":["## 사전 학습 모델\n","\n","* https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip"]},{"cell_type":"code","metadata":{"id":"yoGiYGG1jsHJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711964799106,"user_tz":-540,"elapsed":20332,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"21ae1bda-a7f1-4f3e-f056-3f7c84afd940"},"source":["!wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -O gpt_ckpt.zip\n","!unzip -o gpt_ckpt.zip"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-04-01 09:46:18--  https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip\n","Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n","Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip [following]\n","--2024-04-01 09:46:18--  https://www.dropbox.com/s/raw/nzfa9xpzm4edp6o/gpt_ckpt.zip\n","Reusing existing connection to www.dropbox.com:443.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://uccd38f225c790e3735337e60294.dl.dropboxusercontent.com/cd/0/inline/CQOKYq15ONcOsxtwx1HDhFF2ip9kkzTKqFeyUKDWgp-slt4tBJYdM6w7OVuS7cv9WhxO_4jFOLxUIDFWfbUQuzaeDeEU4qxLzFDWmS_ObuluSCCvVxa7ucjFq7CAsQuuGG-OO5uckJT5dcWLjI5KK7WG/file# [following]\n","--2024-04-01 09:46:19--  https://uccd38f225c790e3735337e60294.dl.dropboxusercontent.com/cd/0/inline/CQOKYq15ONcOsxtwx1HDhFF2ip9kkzTKqFeyUKDWgp-slt4tBJYdM6w7OVuS7cv9WhxO_4jFOLxUIDFWfbUQuzaeDeEU4qxLzFDWmS_ObuluSCCvVxa7ucjFq7CAsQuuGG-OO5uckJT5dcWLjI5KK7WG/file\n","Resolving uccd38f225c790e3735337e60294.dl.dropboxusercontent.com (uccd38f225c790e3735337e60294.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n","Connecting to uccd38f225c790e3735337e60294.dl.dropboxusercontent.com (uccd38f225c790e3735337e60294.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: /cd/0/inline2/CQOEY0l0wSVGkx1PAJPjT3j6igEav-ywJjB81nDhge0hETkQXVtlL-musrAHNLztq3oWwCikikVfk5ks1m1s0nWyyS6wkR3xSzBnK7VUkoEuU8_p4fVgbcEdR-0U9e719LAVXiVU85iS9FvQu4PG5YgIZZJ84g69bZ-JYrhNy64xL-qBHhzU2pfF5A8ydrMJgFb7Q94tqYK18raRP_eLIXQitJDAXYVuk1F1Tw9eDPiDKpR_OXy9wUgtUdBbfit7pJyGa_IqHYcd0CzWga0FQ8l07SfiQCnI3Gj7sQkLRjS8YweMZDQNT2wYAcwlGUfbjn8neNUtfikFIuF2JuqbI8F-0zQaioIIrKQybsI3nluKjmSTKV6nqBpQSGOrSi_LQfQ/file [following]\n","--2024-04-01 09:46:19--  https://uccd38f225c790e3735337e60294.dl.dropboxusercontent.com/cd/0/inline2/CQOEY0l0wSVGkx1PAJPjT3j6igEav-ywJjB81nDhge0hETkQXVtlL-musrAHNLztq3oWwCikikVfk5ks1m1s0nWyyS6wkR3xSzBnK7VUkoEuU8_p4fVgbcEdR-0U9e719LAVXiVU85iS9FvQu4PG5YgIZZJ84g69bZ-JYrhNy64xL-qBHhzU2pfF5A8ydrMJgFb7Q94tqYK18raRP_eLIXQitJDAXYVuk1F1Tw9eDPiDKpR_OXy9wUgtUdBbfit7pJyGa_IqHYcd0CzWga0FQ8l07SfiQCnI3Gj7sQkLRjS8YweMZDQNT2wYAcwlGUfbjn8neNUtfikFIuF2JuqbI8F-0zQaioIIrKQybsI3nluKjmSTKV6nqBpQSGOrSi_LQfQ/file\n","Reusing existing connection to uccd38f225c790e3735337e60294.dl.dropboxusercontent.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 460908853 (440M) [application/zip]\n","Saving to: ‘gpt_ckpt.zip’\n","\n","gpt_ckpt.zip        100%[===================>] 439.56M  57.1MB/s    in 6.6s    \n","\n","2024-04-01 09:46:26 (67.0 MB/s) - ‘gpt_ckpt.zip’ saved [460908853/460908853]\n","\n","Archive:  gpt_ckpt.zip\n","   creating: gpt_ckpt/\n","  inflating: gpt_ckpt/gpt2_kor_tokenizer.spiece  \n","  inflating: gpt_ckpt/config.json    \n","  inflating: gpt_ckpt/tf_model.h5    \n"]}]},{"cell_type":"code","metadata":{"id":"1fjeaDUNjsHP","executionInfo":{"status":"ok","timestamp":1711964807574,"user_tz":-540,"elapsed":278,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["# GPT-2 모델을 TensorFlow의 Keras 모델로 래핑하는 클래스를 정의\n","# 이 클래스는 TensorFlow의 모델 인터페이스를 따르므로 사용자가 GPT-2 모델을 편리하게 사용할 수 있다\n","\n","class GPT2Model(tf.keras.Model):  # 이 클래스는 TensorFlow의 tf.keras.Model을 상속하여 GPT-2 모델을 정의\n","    def __init__(self, dir_path): # __init__ 메서드: 클래스의 생성자로, GPT-2 모델을 불러오고 초기화합니다. dir_path 매개변수를 통해 모델이 저장된 디렉토리 경로를 받습니다.\n","        super(GPT2Model, self).__init__()\n","        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n","\n","    def call(self, inputs):\n","        return self.gpt2(inputs)[0]\n","# call 메서드: 이 메서드는 모델에 입력을 전달하여 출력을 계산\n","# GPT-2 모델의 출력 중 첫 번째 요소를 반환. GPT-2 모델의 출력은 토큰의 확률 분포이며, 이 중 첫 번째 요소는 다음 토큰의 확률 분포를 나타냄."],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qlmm2I0jsHV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711964814551,"user_tz":-540,"elapsed":4290,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"3d34bfe3-a17a-4244-9cfb-af9c408ae686"},"source":["BASE_MODEL_PATH = './gpt_ckpt'\n","gpt_model = GPT2Model(BASE_MODEL_PATH)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n","\n","All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./gpt_ckpt.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"]}]},{"cell_type":"code","metadata":{"id":"g5ilayG3jsHc","executionInfo":{"status":"ok","timestamp":1711965120155,"user_tz":-540,"elapsed":792,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["from gluonnlp.vocab import BERTVocab\n","BATCH_SIZE=16\n","NUM_EPOCHS=10\n","MAX_LEN=30\n","TOKENIZER_PATH='./gpt_ckpt/gpt2_kor_tokenizer.spiece'\n","tokenizer=SentencepieceTokenizer(TOKENIZER_PATH)\n","vocab=nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n","                                       mask_token=None,\n","                                       sep_token=None,\n","                                       cls_token=None,\n","                                       unknown_token='<unk>',\n","                                       padding_token='<pad>',\n","                                       bos_token='<s>',\n","                                       eos_token='</s>'\n","                                       )"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"TaFAxan-jsHg","executionInfo":{"status":"ok","timestamp":1711965123245,"user_tz":-540,"elapsed":325,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["def tf_top_k_top_p_filtering(logits, top_k = 0, top_p = 0.0, filter_value = 99999):\n","    _logits = logits.numpy()\n","    top_k = min(top_k, logits.shape[-1])\n","    if top_k > 0:\n","        indices_to_remove = logit < tf.math.top_k(logits, top_k)[0][..., -1, None]\n","        _logits[indices_to_remove] = filter_value\n","\n","    if top_p > 0.0:\n","        sorted_logits = tf.sort(logits, direction = 'DESCENDING')\n","        sorted_indices = tf.argsort(logits, direction = 'DESCENDING')\n","        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis = -1), axis = -1)\n","\n","        sorted_indices_to_remove = cumulative_probs > top_p\n","        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis = 0)\n","        indeices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n","\n","        _logits[indices_to_remove] = filter_value\n","\n","    return tf.constant([_logits])\n","\n","def generate_sentence(seed_word, model, max_step = 100, greedy = False, top_k = 0, top_p = 0.):\n","    sentence = seed_word\n","    toked = tokenizer(sentence)\n","\n","    for _ in range(max_step):\n","        input_ids = tf.constant([vocab[vocab.bos_token], ]+ vocab[toked])[None, :]\n","        outputs = model(input_ids)[:, -1, :]\n","        if greedy:\n","            gen = vocab.to_tokens(tf.argmax(outputs, axis = -1).numpy().tolist()[0])\n","        else:\n","            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k = top_k, top_p = top_p)\n","            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n","        if gen == '<\\s>':\n","            break\n","        sentence += gen.replace('-', ' ')\n","        toked = tokenizer(sentence)\n","\n","    return sentence"],"execution_count":12,"outputs":[]},{"cell_type":"code","source":["generate_sentence('일부', gpt_model, greedy=True, nbest_size=-1, alpha=0.7)"],"metadata":{"id":"N1e_B14IE74A","executionInfo":{"status":"error","timestamp":1711965194429,"user_tz":-540,"elapsed":337,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"ff6f6668-fdcb-419b-c652-046d8e89b2fd","colab":{"base_uri":"https://localhost:8080/","height":146}},"execution_count":14,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"generate_sentence() got an unexpected keyword argument 'nbest_size'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-6286f60b53ab>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'일부'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: generate_sentence() got an unexpected keyword argument 'nbest_size'"]}]},{"cell_type":"code","metadata":{"id":"q6bhahzWjsHl","colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"status":"error","timestamp":1711965126145,"user_tz":-540,"elapsed":436,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"999737ed-1639-403e-848a-6c120e9f6ab1"},"source":["generate_sentence('일부', gpt_model, greedy = True)"],"execution_count":13,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-d3ea760be3b5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'일부'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-fb2285e46b93>\u001b[0m in \u001b[0;36mgenerate_sentence\u001b[0;34m(seed_word, model, max_step, greedy, top_k, top_p)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtoked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gluonnlp/data/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m         return self._processor.SampleEncodeAsPieces(sample, self._nbest,\n\u001b[0m\u001b[1;32m    562\u001b[0m                                                     self._alpha)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mSampleEncodeAsPieces\u001b[0;34m(self, input, nbest_size, alpha, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mSampleEncodeAsPieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       return self.Encode(input=input, nbest_size=nbest_size, alpha=alpha,\n\u001b[0m\u001b[1;32m    562\u001b[0m                          out_type=str, enable_sampling=True, **kwargs)\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sentencepiece/__init__.py\u001b[0m in \u001b[0;36mEncode\u001b[0;34m(self, input, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[1;32m    501\u001b[0m       if enable_sampling == True and (nbest_size is None or nbest_size == 0 or\n\u001b[1;32m    502\u001b[0m                                       nbest_size == 1 or alpha is None):\n\u001b[0;32m--> 503\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;34m'When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;34m'and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: When enable_sampling is True, We must specify \"nbest_size > 1\" or \"nbest_size = -1\", and \"alpha\". \"nbest_size\" is enabled only on unigram mode ignored in BPE-dropout. when \"nbest_size = -1\" , this method samples from all candidates on the lattice instead of nbest segmentations."]}]},{"cell_type":"code","metadata":{"id":"GW5jmfiejsHr"},"source":["generate_sentence('일부', gpt_model, top_k = 0, top_p = 0.95)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M5yWJea3I7-n"},"source":["## 데이터 준비"]},{"cell_type":"code","metadata":{"id":"CVWJaywYjsHw"},"source":["DATA_IN_PATH = './gpt2/'\n","TRAIN_DATA_FILE = 'finetune_data.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zVXUVGH5jsH0"},"source":["sentences = [s[:-1] for s in open(DATA_IN_PATH + TRAIN_DATA_FILE).readlines()]\n","\n","input_data = []\n","output_data = []\n","\n","for sentence in sentences:\n","    tokens = [vocab[vocab.bos_token], ] + vocab[tokenizer(sentence)] + [vocab[vocab.eos_token], ]\n","    input_data.append(tokens[:-1])\n","    output_data.append(tokens[1:])\n","\n","input_data = pad_sequences(input_data, MAX_LEN, value = vocab[vocab.padding_token])\n","output_data = pad_sequences(output_data, MAX_LEN, value = vocab[vocab.padding_token])\n","\n","input_data = np.array(input_data, dtype = np.int64)\n","output_data = np.array(output_data, dtype = np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O4fmyXIZJMxm"},"source":["## 모델 학습"]},{"cell_type":"code","metadata":{"id":"NDIvpflCjsH5"},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True,\n","                                                            reduction = 'none')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name = 'accuracy')\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype = loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_mean(loss_)\n","\n","def accuracy_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n","    mask = tf.expand_dims(tf.cast(mask, dtype = pred.dtype), axis = -1)\n","    pred *= mask\n","    acc = train_accuracy(real, pred)\n","\n","    return tf.reduce_mean(acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GxW9BUs-jsH9"},"source":["gpt_model.compile(loss = loss_function,\n","                  optimizer = tf.keras.optimizers.Adam(1e-4),\n","                  metrics = [accuracy_function])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"McrNa1eEjsIC"},"source":["history = gpt_model.fit(input_data, output_data,\n","                        batch_size = BATCH_SIZE, epochs = NUM_EPOCHS,\n","                        validation_split = 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFJHOJDqjsIG"},"source":["DATA_OUT_PATH = './data_out'\n","model_name = 'tf2_gpt2_finetuned_model'\n","\n","save_path = os.path.join(DATA_OUT_PATH, model_name)\n","\n","if not os.path.exists(save_path):\n","    os.makedirs(save_path)\n","\n","gpt_model.gpt2.save_pretrained(save_path)\n","\n","loadded_gpt_model = GPT2Model(save_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VBYN8CuxjsIK"},"source":["generate_sentence('일부', gpt_model, greedy = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GeP5zGxHjsIN"},"source":["generate_sentence('일부', gpt_model, top_k = 0, top_p = 0.95)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-BZEEq4mIMhr"},"source":["# GPT2 네이버 영화 리뷰 분류"]},{"cell_type":"markdown","metadata":{"id":"ZTXFkRQYxGa0"},"source":["## 데이터 다운로드"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"ijkw_0U2xGa-"},"source":["import re\n","import urllib.request\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn-white')\n","\n","from transformers import TFGPT2Model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iNs8XHaUxGbQ"},"source":["tf.random.set_seed(111)\n","np.random.seed(111)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jcbcRQKwxGbW"},"source":["## 데이터 준비"]},{"cell_type":"code","metadata":{"id":"wpABh-81xGbW"},"source":["BATCH_SIZE = 32\n","NUM_EPOCHS = 3\n","VALID_SPLIT = 0.1\n","SENT_MAX_LEN = 39"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqPVUEwjxGbb"},"source":["TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n","\n","tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n","                                               mask_token = None,\n","                                               sep_token = '',\n","                                               cls_toeken = None,\n","                                               unknown_token = '',\n","                                               padding_token = '',\n","                                               bos_token = '',\n","                                               eos_token = '<\\s>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0I6dM15ym7uK"},"source":["* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n","* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n"]},{"cell_type":"code","metadata":{"id":"IetCxzkbxGbf"},"source":["train_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\")\n","test_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\")\n","\n","train_data = pd.read_table(train_file)\n","test_data = pd.read_table(test_file)\n","\n","train_data = train_data.dropna()\n","test_data = test_data.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_ZCDWgskiRp"},"source":["train_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVnAFFU-kiny"},"source":["test_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lF8f3VcJxGbj"},"source":["def clean_text(text):\n","    text_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]\", \"\", text)\n","\n","    return text_clean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zuAoVmTGxGbo"},"source":["train_data_sents = []\n","train_data_labels = []\n","\n","for train_sent, train_label in train_data[['document', 'label']].values:\n","    train_tokenized_text = vocab[tokenizer(clean_text(train_sent))]\n","\n","    tokens = [vocab[vocab.bos_token]]\n","    tokens += pad_sequences([train_tokenized_text],\n","                            SENT_MAX_LEN,\n","                            value = vocab[vocab.padding_token],\n","                            padding = 'post').tolist()[0]\n","    tokens += [vocab[vocab.eos_token]]\n","\n","    train_data_sents.append(tokens)\n","    train_data_labels.append(train_label)\n","\n","train_data_sents = np.array(train_data_sents, dtype = np.int64)\n","train_data_labels = np.array(train_data_labels, dtype = np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4w_U2EMQxGbs"},"source":["## 모델 학습"]},{"cell_type":"code","metadata":{"id":"5JYb6XjgxGbu"},"source":["class TFGPT2Classifier(tf.keras.Model):\n","    def __init__(self, dir_path, num_class):\n","        super(TFGPT2Classifier, self).__init__()\n","\n","        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n","        self.num_class = num_class\n","\n","        self.dropout = tf.keras.layers.Dropout(self.gpt2. config.summary_first_dropout)\n","        self.classifier = tf.keras.layers.Dense(self.num_class,\n","                                                kernel_initializer = tf.keras.initializers.TruncatedNormal(stddev = self.gpt2.config.initializer_range),\n","                                                name = 'classifier')\n","\n","    def call(self, inputs):\n","        outputs = self.gpt2(inputs)\n","        pooled_output = outputs[0][:, -1]\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self. classifier(pooled_output)\n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oUfrW5TxGby"},"source":["BASE_MODEL_PATH = './gpt_ckpt'\n","cls_model = TFGPT2Classifier(dir_path = BASE_MODEL_PATH, num_class = 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5OsxKKImxGb1"},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate = 6.25e-5)\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n","cls_model.compile(optimizer = optimizer, loss = loss, metrics = [metric])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRF8D388xGb5"},"source":["model_name = 'tf2_gpt2_naver_movie'\n","\n","es_callback = EarlyStopping(monitor = 'val_accuracy', min_delta = 0.0001, patience = 2)\n","\n","checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","if os.path.exists(checkpoint_dir):\n","    print(\"{} directory already exists\\n\".format(checkpoint_dir))\n","else:\n","    os.makedirs(checkpoint_dir, exist_ok = True)\n","    print(\"{} directory already complete\\n\".format(checkpoint_dir))\n","\n","cp_callback = ModelCheckpoint(checkpoint_path,\n","                              monitor = 'val_accuracy',\n","                              verbose = 1,\n","                              save_best_only = True,\n","                              save_weights_only = True)\n","\n","history = cls_model.fit(train_data_sents, train_data_labels,\n","                        epochs = NUM_EPOCHS,\n","                        batch_size = BATCH_SIZE,\n","                        validation_split = VALID_SPLIT,\n","                        callbacks = [es_callback, cp_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGj4h0l3xGb9"},"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'], '')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(['Loss', 'Validation Loss'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7x-FC6BDxGcB"},"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'], '')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(['Accuracy', 'Validation Accuracy'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YKJx63kSxGcF"},"source":["## 모델 평가"]},{"cell_type":"code","metadata":{"id":"VywcseLrxGcH"},"source":["test_data_sents = []\n","test_data_labels = []\n","\n","for test_sent, test_label in test_data[['document', 'label']].values:\n","    test_tokenized_text = vocab[tokenizer(clean_text(test_sent))]\n","\n","    tokens = [vocab[vocab.bos_token]]\n","    tokens += pad_sequences([test_tokenized_text],\n","                            SENT_MAX_LEN,\n","                            value = vocab[vocab.padding_token],\n","                            padding = 'post').tolist()[0]\n","    tokens += [vocab[vocab.eos_token]]\n","\n","    test_data_sents.append(tokens)\n","    test_data_labels.append(test_label)\n","\n","test_data_sents = np.array(test_data_sents, dtype = np.int64)\n","test_data_labels = np.array(test_data_labels, dtype = np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wj3dRljzxGcP"},"source":["cls_model.load_weights(checkpoint_path)\n","cls_model.evaluate(test_data_sents, test_data_labels, batch_size = 1024)"],"execution_count":null,"outputs":[]}]}