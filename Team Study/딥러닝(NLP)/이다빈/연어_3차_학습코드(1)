{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1L3FXPOL6aBPyTOgHmJUUkYHX-ldm2E5l","timestamp":1711427959624}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tV70fimDW_-G"},"source":["# 개체명 인식(Named Entity Recognition)\n"]},{"cell_type":"markdown","metadata":{"id":"MWA304rM5v8A"},"source":["* 개체명 인식은 텍스트에서 이름을 가진 개체를 인식하는 기술      \n","* 가령, '철수와 영희는 밥을 먹었다'에서 이름과 사물을 추출하는 개체명 인식 모델 결과\n","\n","  철수 - 이름    \n","  영희 - 이름    \n","  밥 - 사물"]},{"cell_type":"markdown","metadata":{"id":"9TwymWiS5fQs"},"source":["## 개체명 인식 - NLTK\n","\n","* https://wikidocs.net/30682"]},{"cell_type":"markdown","metadata":{"id":"WClqugy-7LM4"},"source":["* `nltk` 라이브러리에서는 미리 학습된 개체명 인식 모델을 제공"]},{"cell_type":"markdown","metadata":{"id":"5VP0CghXeLY-"},"source":["### 라이브러리 준비"]},{"cell_type":"code","metadata":{"id":"ovXXNeMb7Xcw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711511539280,"user_tz":-540,"elapsed":4101,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"c3441fd2-b1d8-42e5-9c1a-94f934220411"},"source":["import nltk\n","\n","nltk.download('words')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"O4lxskxmeQJU"},"source":["### 토큰화 및 품사 태깅"]},{"cell_type":"code","metadata":{"id":"3mPuhBfs5qrQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711511539890,"user_tz":-540,"elapsed":617,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"2ff11fa5-d365-4319-dffa-b4bbfc07317d"},"source":["from nltk import word_tokenize, pos_tag, ne_chunk\n","\n","sentence = 'James is working at Disney in London'\n","sentence = pos_tag(word_tokenize(sentence))\n","print(sentence)\n","# 품사태깅을 통해서도 개체명이 어떤것일지 보임"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n"]}]},{"cell_type":"markdown","metadata":{"id":"9DZP3vVveYGs"},"source":["### 개체명 인식"]},{"cell_type":"code","metadata":{"id":"KuDJ4Ur47Gzw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711511539890,"user_tz":-540,"elapsed":11,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"42d649b4-f8c0-46ba-dde6-99b4729ea39c"},"source":["sentence = ne_chunk(sentence)\n","\n","print(sentence)\n","\n","# James : 사람이름\n","# Disney : 기관이름\n","# London : 국가명 의미"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (PERSON James/NNP)\n","  is/VBZ\n","  working/VBG\n","  at/IN\n","  (ORGANIZATION Disney/NNP)\n","  in/IN\n","  (GPE London/NNP))\n"]}]},{"cell_type":"markdown","metadata":{"id":"WvSriP226tSZ"},"source":["## 개체명 인식 - LSTM\n","\n","* https://wikidocs.net/24682"]},{"cell_type":"markdown","metadata":{"id":"vOoJbTJC7xLC"},"source":["* 사용자가 제공되고 있는 개체명 인식 모델과는 다른 개체명을 정의해 사용하는 것이 필요할 수 있음\n","* 직접 개체명 인식 모델을 구성해 학습하고 사용할 수 있음"]},{"cell_type":"markdown","metadata":{"id":"7TZcsoHWee4F"},"source":["### 라이브러리 준비"]},{"cell_type":"code","metadata":{"id":"Z3Pmu8QG64cK","executionInfo":{"status":"ok","timestamp":1711511550124,"user_tz":-540,"elapsed":10240,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["import numpy as np\n","import urllib.request\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import train_test_split"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDgF2DTOeih2"},"source":["### 데이터 준비"]},{"cell_type":"markdown","metadata":{"id":"JZKtNz4OwI1C"},"source":["* 공개된 개체명 인식 데이터셋을 이용\n","  + https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt\n","* 해당 데이터는 단어-개체명 형식으로 이루어져 있으므로 이를 가공해 데이터셋을 생성"]},{"cell_type":"code","metadata":{"id":"IXnAk3SFC87h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711511554842,"user_tz":-540,"elapsed":3291,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"acf985f0-89fa-482c-ca22-c629bb6acbcc"},"source":["tagged_sentences = []\n","sentence = []\n","\n","with urllib.request.urlopen('https://raw.githubusercontent.com/Franck-Dernoncourt/NeuroNER/master/neuroner/data/conll2003/en/train.txt') as f:\n","  for line in f:\n","    line = line.decode('utf-8')\n","    if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\": # line길이 0이거나 DOCSTART로 시작하거나 라인의 첫번째가 개행문자\n","      if len(sentence) >0:\n","        tagged_sentences.append(sentence)\n","        sentence = []\n","      continue\n","    splits = line.strip().split(' ') # 스페이스 기준 split\n","    word = splits[0].lower() # 소문자 변환\n","    sentence.append([word,splits[-1]]) # 단어와 개체명태킹만 쌍으로 묶어서\n","\n","print(len(tagged_sentences)) # 전체 몇개?\n","print(tagged_sentences[0])\n","# 개체명 없는 것도 있음"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["14041\n","[['eu', 'B-ORG'], ['rejects', 'O'], ['german', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['british', 'B-MISC'], ['lamb', 'O'], ['.', 'O']]\n"]}]},{"cell_type":"markdown","metadata":{"id":"iq3VOetBewLN"},"source":["### 데이터 전처리"]},{"cell_type":"markdown","metadata":{"id":"5HTqk1LJwcFm"},"source":["* 단어와 개체명 태그를 분리해서 데이터를 구성"]},{"cell_type":"code","metadata":{"id":"nvsvx1q8C9BP","executionInfo":{"status":"ok","timestamp":1711511613097,"user_tz":-540,"elapsed":409,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["sentences, ner_tags = [], []\n","\n","for tagged_sentence in tagged_sentences:\n","  sentence, tag_info = zip(*tagged_sentence) # tagged_sentence를 가져와서 sentence와 tag_info에 나눠서 분류\n","  sentences.append(list(sentence))\n","  ner_tags.append(list(tag_info))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"flb_gBeGwqdA"},"source":["* 정제 및 빈도 수가 높은 상위 단어들만 추출하기 위해 토큰화 작업"]},{"cell_type":"code","metadata":{"id":"WIFAktdLDJZj","executionInfo":{"status":"ok","timestamp":1711511762406,"user_tz":-540,"elapsed":9,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["# 입력 데이터를 모델이 이해할 수 있는 형태로 변환하는 과정\n","\n","max_words =4000 # 최대 단어 수\n","src_tokenizer = Tokenizer(num_words = max_words, oov_token = 'OOV') # src_tokenizer는 입력 문장을 토큰화\n","# 주어진 텍스트에서 가장 빈번하게 등장하는 단어 max_words - 1개를 선택하고, 나머지는 'OOV' (out of vocabulary)로 대체\n","src_tokenizer.fit_on_texts(sentences) #  각 토크나이저를 주어진 텍스트에 적합화\n","# 단어 사전을 구축\n","\n","\n","\n","tar_tokenizer = Tokenizer() # tar_tokenizer는 NER 태그를 토큰화\n","tar_tokenizer.fit_on_texts(ner_tags)\n","# 레이블 사전을 구축"],"execution_count":9,"outputs":[]},{"cell_type":"code","source":["vocab_size = max_words # 어휘 사전의 크기이며, 이 어휘 크기만큼의 단어가 모델의 입력으로 사용\n","tag_size = len(tar_tokenizer.word_index)+1 #  이 사전의 크기를 나타냅니다. 여기에 1을 더해준 이유는 인덱스가 1부터 시작하기 때문\n","# tar_tokenizer.word_index는 NER 태그에 대한 토크나이저의 단어 인덱스 사전. 각 태그에는 고유한 정수가 할당\n","\n","print(vocab_size) # 어휘사전 크\n","print(tag_size) # tag_size는 NER 태그의 종류 수에 해당"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cdnNAf0lDBnm","executionInfo":{"status":"ok","timestamp":1711511779405,"user_tz":-540,"elapsed":8,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"9792389d-154b-4926-83a4-26a4ab9a3b80"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["4000\n","10\n"]}]},{"cell_type":"code","metadata":{"id":"jsOP0gUdDL6g","executionInfo":{"status":"ok","timestamp":1711513187838,"user_tz":-540,"elapsed":700,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["# 텍스트 데이터를 시퀀스데이터로 변환\n","X_train =  src_tokenizer.texts_to_sequences(sentences)\n","y_train = tar_tokenizer.texts_to_sequences(ner_tags)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q5dr-rJMxFV1"},"source":["* 데이터를 학습에 활용하기 위해 데이터를 배열로 변환\n","* 해당 작업은 토큰화 툴의 `texts_to_sequences()`를 통해 수행"]},{"cell_type":"code","metadata":{"id":"dTUzeqW-DNdv","executionInfo":{"status":"ok","timestamp":1711513236409,"user_tz":-540,"elapsed":727,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["max_len = 70\n","X_train = pad_sequences(X_train, padding = 'post', maxlen = max_len)\n","y_train = pad_sequences(y_train, padding = 'post', maxlen =max_len)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"klstjOu4xQmL"},"source":["* 학습에 투입할 때는 동일한 길이를 가져야 하므로, 지정해둔 최대 길이에 맞춰 모든 데이터를 동일한 길이로 맞춰줌\n","* 일반적으로 길이를 맞출 때는 모자란 길이만큼 0을 추가\n"]},{"cell_type":"code","metadata":{"id":"jwvNulj2DO9Z","executionInfo":{"status":"ok","timestamp":1711513312310,"user_tz":-540,"elapsed":6,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = .2, random_state=111)\n","\n","y_train = to_categorical(y_train, num_classes = tag_size)\n","y_test = to_categorical(y_test, num_classes=tag_size) # num_classes는 태그의 종류 수\n","\n","# to_categorical: NER 태그를 원-핫 인코딩. 이는 각 NER 태그를 다차원 벡터로 변환하여 모델이 이해할 수 있도록 합니다."],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vYK8UDkxxefx"},"source":["* 훈련, 실험 데이터 분리 및 원 핫 인코딩을 시행"]},{"cell_type":"code","metadata":{"id":"IVtO_DApDR64","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711513342796,"user_tz":-540,"elapsed":8,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"894a36ac-55d8-48b6-a907-9f690f8d7e0c"},"source":["print(X_train.shape) # (샘플 수, 시퀀스의 길이)\n","print(y_train.shape) # (샘플 수, 시퀀스의 길이, tag 수)\n","print(X_test.shape)\n","print(y_test.shape)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["(11232, 70)\n","(11232, 70, 10)\n","(2809, 70)\n","(2809, 70, 10)\n"]}]},{"cell_type":"markdown","metadata":{"id":"CGqT1AGPxjKV"},"source":["* 최종적으로 생성된 데이터셋의 크기는 다음과 같음"]},{"cell_type":"code","metadata":{"id":"cxrX6hkIDTU4","executionInfo":{"status":"ok","timestamp":1711514190286,"user_tz":-540,"elapsed":7,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["from keras.models import Sequential   # 신경망 모델을 순차적으로 정의, 이 모델은 순차적으로 층을 쌓아서 구축\n","from keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed\n","from keras.optimizers import Adam"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8h9L6H-e0Vl"},"source":["### 모델 구축 및 학습"]},{"cell_type":"markdown","metadata":{"id":"eMLRWTzOxzf5"},"source":["* 모델 구축에는 `keras`를 이용\n","* 해당 작업에 필요한 함수들을 추가로 import"]},{"cell_type":"markdown","metadata":{"id":"A0r8yR7nx84S"},"source":["모델의 구성\n","\n","1. 입력을 실수 벡터로 임베딩\n","2. 양방향 LSTM 구성\n","3. Dense layer를 통한 각 태그에 속할 확률 예측\n","\n","`TimeDistributed`는 상위 layer의 출력이 step에 따라 여러 개로 출력되어 이를 적절하게 분배해주는 역할"]},{"cell_type":"code","metadata":{"id":"AQzpED-KDW3f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711516035107,"user_tz":-540,"elapsed":2790,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"9e2b4f53-8261-4bb9-f100-c2d327a84839"},"source":["model = Sequential()\n","model.add(Embedding(input_dim = vocab_size, output_dim = 128, input_length = max_len, mask_zero = True)) #  입력이 0으로 패딩된 경우에 해당 입력을 무시하도록 마스킹하는 옵션\n","model.add(Bidirectional(LSTM(256, return_sequences = True)))\n","model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n","# 시간 분배(Dense) 레이어를 추가하여 각 시간 단계의 출력에 독립적으로 밀집 레이어를 적용\n","# 출력은 NER 태그의 종류 수에 대한 확률 분포를 나타냄\n","# 활성화 함수는 softmax 함수를 사용하여 출력을 확률로 변환\n","\n","model.summary()"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 70, 128)           512000    \n","                                                                 \n"," bidirectional (Bidirection  (None, 70, 512)           788480    \n"," al)                                                             \n","                                                                 \n"," time_distributed (TimeDist  (None, 70, 10)            5130      \n"," ributed)                                                        \n","                                                                 \n","=================================================================\n","Total params: 1305610 (4.98 MB)\n","Trainable params: 1305610 (4.98 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"w0VhAso8DYc4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711516918213,"user_tz":-540,"elapsed":510172,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"c3c8a816-5b0d-403b-a18b-104fbf2dc5df"},"source":["model.compile(loss = 'categorical_crossentropy', # 다중 클래스 분류 문제에 적합한 손실 함수로, 원-핫 인코딩된 레이블을 사용\n","              optimizer = Adam(0.001), # 학습률(learning rate)은 0.001로 설정\n","              metrics = ['accuracy']) # 학습 및 평가 지표로 정확도를 사용\n","model.fit(X_train, y_train, batch_size=128, epochs=3, validation_data = (X_test, y_test)) # batchsize 한 번에 처리되는 샘플의 수\n","\n","# 컴파일 및 모델 학습과정에의 손실과 정확도 모니터링하여 모델의 성능평가"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","88/88 [==============================] - 143s 2s/step - loss: 0.8967 - accuracy: 0.8241 - val_loss: 0.6228 - val_accuracy: 0.8321\n","Epoch 2/3\n","88/88 [==============================] - 171s 2s/step - loss: 0.4745 - accuracy: 0.8557 - val_loss: 0.3742 - val_accuracy: 0.8822\n","Epoch 3/3\n","88/88 [==============================] - 145s 2s/step - loss: 0.3196 - accuracy: 0.9027 - val_loss: 0.2749 - val_accuracy: 0.9157\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x789cf61f0d00>"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"FrpUv_mxyqzR"},"source":["* 모델 컴파일 및 학습 진행, 평가"]},{"cell_type":"code","metadata":{"id":"ARUqZJwHDZuT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711517295835,"user_tz":-540,"elapsed":12657,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"52f45171-c4f8-4805-aedc-fdbb7011ec0e"},"source":["loss, accuracy = model.evaluate(X_test, y_test) # 손실(loss) 값과 선택한 메트릭(여기서는 정확도)을 반환\n","print(\"손실:\", loss)\n","print(\"정확도:\", accuracy)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["88/88 [==============================] - 12s 139ms/step - loss: 0.2766 - accuracy: 0.9157\n","손실: 0.27657365798950195\n","정확도: 0.9157443046569824\n"]}]},{"cell_type":"markdown","metadata":{"id":"OO13spS4fA27"},"source":["### 학습한 모델을 통한 예측"]},{"cell_type":"markdown","metadata":{"id":"KeLrlYTOyy3P"},"source":["* 예측을 확인하기 위해서 인덱스를 단어로 변환해줄 사전이 필요\n","* 사전은 토큰화 툴의 사전을 이용"]},{"cell_type":"code","metadata":{"id":"fQx8MLCZvdI0","executionInfo":{"status":"ok","timestamp":1711517018779,"user_tz":-540,"elapsed":678,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["idx2word = src_tokenizer.index_word # 입력 문장의 정수 인덱스를 해당하는 단어로 변환하는 딕셔너리\n","idx2ner = tar_tokenizer.index_word # 출력 NER 태그의 정수 인덱스를 해당하는 태그로 변환하는 딕셔너리\n","idx2ner[0] = 'PAD' # NER 태그의 정수 인덱스 0을 'PAD'로 변환하여 패딩을 나타내는 특별한 토큰으로 설정"],"execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4EQgmTIzBHp"},"source":["* 예측 시각화"]},{"cell_type":"code","metadata":{"id":"BsHL9I4yDgpW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711517411096,"user_tz":-540,"elapsed":737,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"238de567-ef91-4138-acb7-250af3103274"},"source":["i = 60\n","y_predicted = model.predict(np.array([X_test[i]]))\n","y_predicted = np.argmax(y_predicted, axis=-1)\n","true = np.argmax(y_test[i], -1)\n","\n","print(\"{:15}|{:5}|{}\".format(\"단어\",\"실제값\",\"예측값\"))\n","print(\"-\" * 34)\n","\n","for w,t,pred in zip(X_test[i], true, y_predicted[0]):\n","  if w !=0:\n","    print(\"{:17}: {:7} {}\".format(idx2word[w], idx2ner[t].upper(), idx2ner[pred].upper()))\n","\n","# 각 단어에 대해 실제 NER 태그와 모델의 예측된 NER 태그가 비교되고 출력됩니다. 이를 통해 모델의 예측이 얼마나 정확한지를 확인"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 94ms/step\n","단어             |실제값  |예측값\n","----------------------------------\n","feyenoord        : B-ORG   B-ORG\n","midfielder       : O       O\n","OOV              : B-PER   B-PER\n","van              : I-PER   B-PER\n","OOV              : I-PER   I-PER\n","was              : O       O\n","also             : O       O\n","named            : O       O\n","to               : O       O\n","make             : O       O\n","his              : O       O\n","debut            : O       O\n","in               : O       O\n","the              : O       O\n","OOV              : O       O\n","squad            : O       O\n",".                : O       O\n"]}]},{"cell_type":"markdown","source":["##### 주어진 코드는 모델을 사용하여 특정 테스트 데이터 샘플에 대한 예측을 수행하고, 예측 결과를 출력하는 것입니다. 이를 통해 모델이 특정 문장의 각 단어에 대해 예측한 NER 태그를 확인할 수 있습니다.\n","\n","* model.predict: 모델을 사용하여 입력 데이터에 대한 예측을 수행합니다. 입력 데이터는 X_test[i]로 제공되며, 이는 하나의 테스트 샘플을 나타냅니다.\n","* np.argmax: 각 단어에 대한 예측된 NER 태그 중 가장 확률이 높은 것을 선택합니다.\n","* true: 실제 태그에 해당하는 정수를 가져옵니다.\n","* idx2word, idx2ner: 정수를 단어 또는 NER 태그로 변환하기 위한 딕셔너리입니다.\n","그런 다음, 예측된 NER 태그와 실제 NER 태그를 출력하여 비교"],"metadata":{"id":"LorTACAYZDrM"}}]}