{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1NlSZKwocO_9Z6Tbw7X4v9YsISHAZfJlK","timestamp":1710514579374}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"meRAQbe3ArB7"},"source":["# 문서 분류(Document Classification)"]},{"cell_type":"markdown","metadata":{"id":"myxaNrtIl7Z0"},"source":["## 데이터 준비"]},{"cell_type":"markdown","metadata":{"id":"e85EZuHb-Z9f"},"source":["* 문서 분류에 필요한 데이터는 `scikit-learn`이 제공하는 20개의 주제를 가지는 뉴스그룹 데이터를 사용\n","* 텍스트는 `CounterVectorizer`를 거쳐 DTM으로 변환\n","* DTM 행렬은 문서에 등장하는 단어들을 빈도 수 별로 표현한 행렬\n"]},{"cell_type":"code","metadata":{"id":"AYsGQgxCl-kQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710652229939,"user_tz":-540,"elapsed":8773,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"ca6b08f9-19a1-4692-eaf0-2f950bf56e3f"},"source":["from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","\n","news = fetch_20newsgroups()\n","\n","x = news.data\n","y = news.target\n","\n","cv = CountVectorizer()\n","x = cv.fit_transform(x)\n","\n","x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)\n","print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(7919, 130107) (7919,) (3395, 130107) (3395,)\n"]}]},{"cell_type":"code","metadata":{"id":"4iRARmcl_EmI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710652395068,"user_tz":-540,"elapsed":12,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"9890fd17-88f0-45f8-eee1-c0ee27c98b36"},"source":["print(x_train[0])\n","# 0번째 문서에서 index 56972라는 단어는 1번 등장\n","# 0번째 문서에서 index 50527라는 단어는 3번 등장"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  (0, 56979)\t1\n","  (0, 50527)\t3\n","  (0, 124031)\t1\n","  (0, 85354)\t3\n","  (0, 111322)\t1\n","  (0, 123984)\t5\n","  (0, 68532)\t7\n","  (0, 114731)\t1\n","  (0, 90379)\t1\n","  (0, 118983)\t1\n","  (0, 89362)\t11\n","  (0, 76032)\t1\n","  (0, 123292)\t3\n","  (0, 65798)\t3\n","  (0, 90774)\t2\n","  (0, 114455)\t18\n","  (0, 90686)\t1\n","  (0, 68766)\t1\n","  (0, 115475)\t4\n","  (0, 37433)\t1\n","  (0, 123796)\t1\n","  (0, 99822)\t1\n","  (0, 66608)\t10\n","  (0, 27436)\t1\n","  (0, 73201)\t2\n","  :\t:\n","  (0, 34292)\t1\n","  (0, 118482)\t1\n","  (0, 62973)\t1\n","  (0, 101036)\t1\n","  (0, 64091)\t1\n","  (0, 53787)\t1\n","  (0, 27803)\t4\n","  (0, 63902)\t2\n","  (0, 64094)\t2\n","  (0, 36590)\t1\n","  (0, 40105)\t1\n","  (0, 50997)\t1\n","  (0, 100390)\t1\n","  (0, 30176)\t1\n","  (0, 84914)\t1\n","  (0, 104277)\t1\n","  (0, 35638)\t1\n","  (0, 67998)\t2\n","  (0, 81219)\t1\n","  (0, 45882)\t1\n","  (0, 68497)\t1\n","  (0, 58609)\t1\n","  (0, 80942)\t1\n","  (0, 123291)\t1\n","  (0, 59699)\t1\n"]}]},{"cell_type":"markdown","metadata":{"id":"JGM2WbEdAsGL"},"source":["## scikit-learn을 이용한 문서 분류"]},{"cell_type":"code","metadata":{"id":"M7g4PrqerCkj"},"source":["from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jnww8xssA-FL"},"source":["### 로지스틱 회귀(Logistic Regression)"]},{"cell_type":"markdown","metadata":{"id":"mgUTDK7o9wC6"},"source":["* 로지스틱 회귀는 이름에 회귀라는 단어가 들어가지만, 가능한 클래스가 2개인 이진분류를 위한 모델\n","* 로지스틱 회귀의 예측 함수 정의\n","<br>\n"," $\\sigma (x)$ = $\\frac{1}{1+e^{-x}} $\n"," <br>\n"," $\\widehat{y}$=$\\sigma (\\omega _0+\\omega +\\dots{}+\\omega_px_p)$\n","* σ:시그모이드 함수\n","* 로지스틱 회귀 모델은 선형 회귀 모델에 시그모이드 함수를 적용\n","* 로지스탁 회귀의 학습 목표는 다음과 같은 목적 함수를 최소화 하는 파라미터 𝜔 를 찾는 것\n","$BinaryCrossEntropy =  -\\frac{1}{N}  \\sum _{i=1}^{N} y_ilog(\\widehat{y}_i)+(1-y_i)log(1-\\widehat{y}_i)$"]},{"cell_type":"markdown","source":["* Logistic Regression은 특성상 다중 분류에는 적합하지 않음"],"metadata":{"id":"2NOAXmdE9Zhl"}},{"cell_type":"code","metadata":{"id":"rk3jq9p9DCcv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710654784467,"user_tz":-540,"elapsed":106650,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"b9f87217-251d-4169-fd04-0654c528e8bc"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","LR = LogisticRegression()\n","LR.fit(x_train, y_train) #학습\n","pred = LR.predict(x_test) #predict\n","acc = accuracy_score(pred, y_test) #실제값과 예측값의 정확도\n","print(acc)\n","#0.86정도 -> 다중회귀 특성에 안맞기 때문에 사용하기 어렵"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8568483063328424\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]},{"cell_type":"markdown","source":["### 서포트 벡터 머신(Support Vector Machines)\n","* 회귀,분류,이상치 탐지 등에 사용되는 지도학습 방법\n","* 클래스 사이의 경계에 위치한 데이터 포인트를 서포트 벡터(support vector)라고 함\n","* 각 서포트 벡터가 클래스 사이의 결정 경계를 구분하는데 얼마나 중요한지를 학습\n","* 각 서포트 벡터 사이의 마진이 가장 큰 방향으로 학습\n","*서포트 벡터 까지의 거리와 서포트 벡터의 중요도를 기반으로 예측 수행"],"metadata":{"id":"ECvcA--v9wo1"}},{"cell_type":"code","source":["# 머신러닝 방법 중 분류를 잘함\n","rom sklearn import svm\n","\n","SVM = svm.SVC(kernel = 'linear')\n","SVM.fit(x_train, y_train)\n","pred = SVM.predict(x_test)\n","acc = accuracy_score(pred, y_test)\n","print(acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XVHlO1dB-eop","executionInfo":{"status":"ok","timestamp":1710655104647,"user_tz":-540,"elapsed":102600,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"8722fb97-9041-4b7e-ff08-9891c810cfc4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8073637702503682\n"]}]},{"cell_type":"markdown","metadata":{"id":"M4g1mEefA-SU"},"source":["### 나이브 베이스 분류기 (Naive Bayes Classification)\n","*베이즈 정리를 적용한 확률적 분류 알고리즘\n","* 모든 특성들이 독립임을 가정 (naive 가정)\n","* 입력 특성에 따라 3개의 분류기 존재\n"," * 가우시안 나이브 베이즈 분류기\n"," * 베르누이 나이브 베이즈 분류기\n"," * 다항 나이브 베이즈 분류기"]},{"cell_type":"markdown","metadata":{"id":"qcIkL0Bx-AIG"},"source":["#### DTM을 이용한 Naive Bayes"]},{"cell_type":"code","metadata":{"id":"ogbxoPS0DMTd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710655429497,"user_tz":-540,"elapsed":551,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"061834f6-cb07-4f53-e673-48cae9400ba3"},"source":["from sklearn.naive_bayes import MultinomialNB # 다항 나이브 베이즈 분류기 이용\n","\n","NB = MultinomialNB()\n","NB.fit(x_train, y_train)\n","pred = NB.predict(x_test)\n","acc = accuracy_score(pred, y_test)\n","print(acc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8206185567010309\n"]}]},{"cell_type":"markdown","metadata":{"id":"xZC7kjWt961H"},"source":["#### tf-idf를 이용한 정확도 향상"]},{"cell_type":"code","metadata":{"id":"c2j7cZc71yiJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710655576220,"user_tz":-540,"elapsed":673,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"16e0e093-eba7-461f-a2a6-17b44e9ab007"},"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","\n","tfidf = TfidfTransformer()\n","x_train_tf = tfidf.fit_transform(x_train)\n","x_test_tf = tfidf.fit_transform(x_test)\n","\n","NB.fit(x_train_tf,y_train)\n","pred = NB.predict(x_test_tf)\n","acc = accuracy_score(pred, y_test)\n","print(acc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8170839469808542\n"]}]},{"cell_type":"markdown","source":["### 결정 트리(Decision Tree)\n","* 분류와 회귀에 사용되는 지도 학습 방법\n","* 데이터 특성으로부터 추론된 결정 규칙을 통해 값을 예측\n","* if-then-else 결정 규칙을 통해 데이터 학습\n","* 트리의 깊이가 깊을수록 복잡한 모델"],"metadata":{"id":"XveOBwuUBQy0"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","\n","DT = DecisionTreeClassifier()\n","DT.fit(x_train, y_train)\n","pred = DT.predict(x_test)\n","acc = accuracy_score(pred, y_test)\n","print(acc)\n","# 현재 데이터에서는 규칙을 찾기 어려움."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xhPHyuIfBgmd","executionInfo":{"status":"ok","timestamp":1710655765172,"user_tz":-540,"elapsed":31311,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"cbf6b0fe-2365-4634-e7c0-a3a68b95a2d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.614138438880707\n"]}]},{"cell_type":"markdown","metadata":{"id":"UeEPOjxxA-my"},"source":["### XGBoost\n","* 트리 기반의 앙상블 기법\n","* 분류에 있어서 다른 알고리즘보다 좋은 예측 성능을 보여줌\n","* XGBoost는 GBM기반이지만, GBM의 단점인 느린 수행 시간과 과적합 규제 부재 등의 문제를 해결\n","* 병령 CPU환경에서 빠르게 학습 가능"]},{"cell_type":"code","metadata":{"id":"m4TAgLK5ECCi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710656023951,"user_tz":-540,"elapsed":72130,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"26f56590-c37a-4e4a-808f-f47b208e0b02"},"source":["from xgboost import XGBClassifier\n","\n","xgb = XGBClassifier(n_estimators = 30, learning_rate = 0.05, max_depth = 3)\n","xgb.fit(x_train, y_train)\n","pred = xgb.predict(x_test)\n","acc = accuracy_score(pred, y_test)\n","print(acc)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6963181148748159\n"]}]},{"cell_type":"markdown","metadata":{"id":"o1b4PWPBAWwn"},"source":["##교차 검증"]},{"cell_type":"markdown","metadata":{"id":"bd7AexinGlYP"},"source":["* 일반 검증은 학습 데이터가 테스트 데이터로 사용되지 않음\n","* 교차 검증은 데이터를 n개의 집합으로 나누어 정확도를 계산해 학습 데이터로 사용된 데이터도 테스트 데이터로 사용\n","* 교차 검증을 사용하면 일반 검증보다 모델의 일반화가 잘 되어 있는지 평가 가능\n","* 앞서 구성한 나이브 베이즈 모델을 교차 검증"]},{"cell_type":"code","metadata":{"id":"ZzaiICzaHrI7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710656104431,"user_tz":-540,"elapsed":1705,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"ef33a5fb-a0d5-4f9f-89a8-bdc28d258ac0"},"source":["from sklearn.model_selection import cross_val_score\n","\n","scores = cross_val_score(NB, x, y, cv=5)\n","print(scores, scores.mean())\n","# 83.34정도"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.83870968 0.83826779 0.82368537 0.83031374 0.83642794] 0.833480903927519\n"]}]},{"cell_type":"markdown","metadata":{"id":"qVQBNi5lIn3X"},"source":["* 교차 검증을 통해 일반 검증보다 좀 더 일반화된 모델 생성 가능\n","* 교차 검증은 일반 검증에 비해 n번 검증을 해 비용이 더 많이 소요"]},{"cell_type":"markdown","metadata":{"id":"uqIU3r_9AZQm"},"source":["## 정밀도와 재현률"]},{"cell_type":"markdown","metadata":{"id":"5C0ZPgKLJSvt"},"source":["\n","* 정밀도(precision)는 양성 클래스(정답)으로 예측한 샘플이 양성 클래스일 확률을 의미\n","* 모델이 얼마나 양성 클래스를 잘 예측하는지를 나타냄\n","* 재현률(recall)은 양성 클래스인 샘플에서 모델이 양성 클래스로 예측한 샘플 비율을 의미하며, 모델이 얼마나 실제 상황을 재현하는지를 나타냄\n","* 정밀도와 재현율의 가중조화평균인 F1-score라는 지표는 정확도에 비해 더 효과적인 모델 분석 지표로 알려져 있음\n","* 직접 계산할 수도 있으나, scikit-learn은 이를 편리하게 계산해주는 함수를 제공"]},{"cell_type":"markdown","metadata":{"id":"Y3hObzsrNuzF"},"source":["* 다중 클래스 분류 문제에서 정밀도와 재현률을 계산할 때는 클래스간의 지표를 어떻게 합칠지 지정 필요\n","\n","  * None - 클래스간 지표를 합치지 말고 그대로 출력\n","  * micro - 정밀도와 재현률이 같음, 이로 인해 f1-score도 정밀도, 재현률과 동일\n","  * macro - 클래스간 지표를 단순 평균한 값\n","  * weighted - 클래스간 지표를 가중 평균한 값"]},{"cell_type":"code","metadata":{"id":"98npJWW8J9Px","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710656380162,"user_tz":-540,"elapsed":307,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"49de7c52-aa9f-486c-b05e-c05ef4f402bd"},"source":["from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","precision = precision_score(pred, y_test, average='micro')\n","recall = recall_score(pred, y_test, average='micro')\n","f1 = f1_score(pred, y_test, average='micro')\n","\n","print(precision, recall, f1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6963181148748159 0.6963181148748159 0.6963181148748159\n"]}]},{"cell_type":"code","metadata":{"id":"3JPcMoD0NQi6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710656410164,"user_tz":-540,"elapsed":340,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"f362961b-534b-460c-81c6-18f46dc4ff64"},"source":["precision = precision_score(pred, y_test, average='macro')\n","recall = recall_score(pred, y_test, average='macro')\n","f1 = f1_score(pred, y_test, average='macro')\n","\n","print(precision, recall, f1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6892998263425496 0.7337019341031777 0.704106973631445\n"]}]},{"cell_type":"markdown","metadata":{"id":"QY3s-EXdArpC"},"source":["## 그리드 검색을 이용한 파라미터 최적화"]},{"cell_type":"markdown","metadata":{"id":"Z0rY-TxKCsOi"},"source":["* 그리드 검색을 사용하면 분류기에 사용하는 파라미터 최적화 가능\n","* 그리드 검색을 통해 앞서 구성한 나이브 베이즈 모델의 'alpha' 파라미터를 최적화시키는 예제"]},{"cell_type":"markdown","metadata":{"id":"jOkeA7siDdvF"},"source":["* `estimator`: 사용 모델 객체     \n","* `param_grid`: 사용 객체:지정 파라미터 리스트로 구성된 딕셔너리    \n","* `scoring`: 최적화하고자 하는 성능 지표   \n","* `cv`: 교차 검증 분할 개수      "]},{"cell_type":"code","metadata":{"id":"cCheUO9YBgRi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710656739390,"user_tz":-540,"elapsed":18740,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"26313104-6736-43f5-a69e-0aac60cf7dbb"},"source":["from sklearn.model_selection import GridSearchCV\n","\n","GS = GridSearchCV(estimator = NB, param_grid={'alpha':[0.001, 0.01, 0.1, 1,]}, scoring= 'accuracy' ,cv=10)\n","GS.fit(x,y)\n","\n","print(GS.best_score_)\n","print(GS.best_params_)\n","# 성능이 88퍼까지 올라감. 파라미터값은 0.001이 가장 좋은 값"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8897820965842167\n","{'alpha': 0.001}\n"]}]}]}