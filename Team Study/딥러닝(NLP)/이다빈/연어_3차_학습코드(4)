{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1-chrVi5GtTwxd6KNYthnX8hvDVFwHnhG","timestamp":1711427921925}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ttMLpgHe0qS0"},"source":["# BERT(Bidirectional Encoder Representations from Transformers)\n","\n","* 참고: https://ebbnflow.tistory.com/151\n","* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2\n","\n","\n","---\n","트랜스포머의 확장 개념\n"]},{"cell_type":"markdown","metadata":{"id":"n9UVaxPPBXup"},"source":["## Input Representation\n","\n","* 3가지의 입력 임베딩(Token, Segment, Position 임베딩)의 합으로 구성\n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbABsUL%2FbtqzmTU7OLm%2FYwK6JLhNfTYvxkiFzkfkCK%2Fimg.png)"]},{"cell_type":"markdown","metadata":{"id":"4BZpgYyoBlqW"},"source":["### Token Embeddings\n","\n","* Word Piece 임베딩 방식 사용\n","* 자주 등장하면서 가장 긴 길이의 sub-word를 하나의 단위로 생성\n","* 즉, 자주 등장하는 sub-word은 그 자체가 단위가 되고, 자주 등장하지 않는 단어(rare word)는 sub-word로 쪼개짐\n","* 기존 워드 임베딩 방법은 Out-of-vocabulary (OOV) 문제가 존재하며, 희귀 단어, 이름, 숫자나 단어장에 없는 단어에 대한 학습, 번역에 어려움이 있음\n","* Word Piece 임베딩은 모든 언어에 적용 가능하며, sub-word 단위로 단어를 분절하므로 OOV 처리에 효과적이고 정확도 상승효과도 있음\n"]},{"cell_type":"markdown","metadata":{"id":"y_Xa5v1iCBzE"},"source":["### Sentence Embeddings\n","\n","* BERT는 두 개의 문장을 문장 구분자([SEP])와 함께 결합\n","* 입력 길이의 제한으로 두 문장은 합쳐서 512 subword 이하로 제한\n","* 입력의 길이가 길어질수록 학습시간은 제곱으로 증가하기 때문에 적절한 입력 길이 설정 필요\n","* 한국어는 보통 평균 20 subword로 구성되고 99%가 60 subword를 넘지 않기 때문에 입력 길이를 두 문장이 합쳐 128로 해도 충분\n","* 간혹 긴 문장이 있으므로 우선 입력 길이 128로 제한하고 학습한 후, 128보다 긴 입력들을 모아 마지막에 따로 추가 학습하는 방식을 사용\n"]},{"cell_type":"markdown","metadata":{"id":"6dloBbq6CD7v"},"source":["### Position Embedding\n","\n","* BERT는 저자의 이전 논문인 Transformer 모델을 착용\n","* Transformer은 주로 사용하는 CNN, RNN 모델을 사용하지 않고 Self-Attention 모델을 사용\n","* Self-Attention은 입력의 위치에 대해 고려하지 못하므로 입력 토큰의 위치 정보가 필요\n","* Transformer 에서는 Sinusoid 함수를 이용한 Positional encoding을 사용하였고, BERT에서는 이를 변형하여 Position encoding을 사용\n","* Position encoding은 단순하게 Token 순서대로 0, 1, 2, ...와 같이 순서대로 인코딩\n"]},{"cell_type":"markdown","metadata":{"id":"AF-JatE-CF7P"},"source":["### 임베딩 취합\n","\n","* BERT는 위에서 소개한 3가지의 입력 임베딩(Token, Segment, Position 임베딩)을 취합하여 하나의 임베딩 값으로 생성\n","* 임베딩의 합에 Layer Normalization과 Dropout을 적용하여 입력으로 사용"]},{"cell_type":"markdown","metadata":{"id":"CWYtaq0qCq6P"},"source":["## 언어 모델링 구조(Pre-training BERT)\n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbg5SlP%2FbtqzntBU7Uj%2FKHWiKI4zKgb8FqLzAYAusK%2Fimg.png)"]},{"cell_type":"markdown","metadata":{"id":"8qVY1sl5C8dD"},"source":["### 언어 모델링 데이터\n","\n","* BERT는 총 3.3억 단어(8억 단어의 BookCorpus 데이터와 25억 단어의 Wikipedia 데이터)의 거대한 말뭉치를 이용하여 학습\n","* 거대한 말뭉치를 MLM, NSP 모델 적용을 위해 스스로 라벨을 만들고 수행하므로 준지도학습(Semi-supervised)이라고 함\n","* Wikipedia와 BookCorpus를 정제하기 위해 list, table, header를 제거\n","* 문장의 순서를 고려해야 하므로 문단 단위로 분리하였고 많은 데이터 정제 작업을 수행"]},{"cell_type":"markdown","metadata":{"id":"QTASWRoyDHKy"},"source":["### 모델 구조\n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbL28Ok%2FbtqznO6UmYw%2Fe0mFyA814Pvj4kltVxKls0%2Fimg.png)\n","\n","\n","* BERT 모델은 Transformer를 기반으로 함\n","* Transformer 모델 구조는 인코더-디코더 모델이며 번역 도메인에서 최고 성능을 기록\n","* 기존 인코더-디코더 모델들과 다르게 Transformer는 CNN, RNN을 이용하지 않고 Self-attention이라는 개념을 도입\n","* BERT는 Transformer의 인코더-디코더 중 인코더만 사용하는 모델"]},{"cell_type":"markdown","metadata":{"id":"fq2CX1-sDZWz"},"source":["### MLM(Masked Language Model)\n","\n","* 입력 문장에서 임의로 Token을 마스킹(masking), 그 Token을 맞추는 방식인 MLM 학습 진행\n","* 문장의 빈칸 채우기 문제를 학습"]},{"cell_type":"markdown","metadata":{"id":"EkKUJCOJEDIN"},"source":["* 생성 모델 계열은(예를들어 GPT) 입력의 다음 단어를 예측\n","* MLM은 문장 내 랜덤한 단어를 마스킹 하고 이를 예측\n","* 입력의 15% 단어를 [MASK] Token으로 바꿔주어 마스킹\n","* 이 때 80%는 [MASK]로 바꿔주지만, 나머지 10%는 다른 랜덤 단어로, 또 남은 10%는 바꾸지 않고 그대로 둠\n","* 이는 미세 조정 시 올바른 예측을 돕도록 마스킹에 노이즈를 섞음\n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FLMyXN%2Fbtqzl4Ql7sH%2FykzRZNWkc6rcb8ffU5Nrm1%2Fimg.png)"]},{"cell_type":"markdown","metadata":{"id":"SYxZ3AV5EMt5"},"source":["* 아래 그림은 MLM의 학습 과정\n","* 입력 단어의 15%가 [MASK]로 대체된 입력이 들어가고, MLM은 [MASK]가 어떤 단어인지를 예측\n","* BERT의 Token 임베딩은 Word Piece 임베딩 방식을 사용하고, Word piece의 단어수는 30522 단어\n","* 3만 단어 중 [MASK]에 들어갈 단어를 찾는 것이므로 MLM의 출력인 Softmax의 클래스는 3만개\n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fc0lfDS%2FbtqzmTOp4JK%2FXkDq157Mw7MnycHeC2NAx1%2Fimg.png)"]},{"cell_type":"markdown","metadata":{"id":"ZbIouOAAEW3P"},"source":["### NSP(Next Sentence Prediction)\n","\n","* NSP는 두 문장이 주어졌을 때 두 번째 문장이 첫 번째 문장의 바로 다음에 오는 문장인지 여부를 예측하는 방식\n","* 두 문장 간 관련이 고려되어야 하는 NLI와 QA의 파인튜닝을 위해 두 문장이 연관이 있는지를 맞추도록 학습\n","* 아래 그림은 NSP의 입력 예시\n","* 위에서 설명한 MLM과 동시에 NSP도 적용된 문장들\n","* 첫 번째 문장과 두 번째 문장은 [SEP]로 구분\n","* 두 문장이 실제로 연속하는지는 50% 비율로 참인 문장과, 50%의 랜덤하게 추출된 상관 없는 문장으로 구성\n","* 이 학습을 통해 문맥과 순서를 언어모델이 학습 가능\n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FmRPzz%2Fbtqzps28Eyd%2F2ak5AHBLlk1jXHnOgGwyMK%2Fimg.png)"]},{"cell_type":"markdown","metadata":{"id":"5o2iHcTxEpPf"},"source":["* 아래 그림은 NSP의 학습 방법\n","* 연속 문장인지, 아닌지만 판단하면 되므로 Softmax의 출력은 2개이고 3만개의 출력을 갖는 MLM에 비해 빠르게 학습\n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlyapH%2FbtqzmkrVtki%2FUUqjexLh7Lt4ZwMVpjIBJ1%2Fimg.png)"]},{"cell_type":"markdown","metadata":{"id":"WDPn-eM-Eycz"},"source":["## 학습된 언어모델 전이학습(Transfer Learning)\n","\n","* 파인 튜닝은 학습된 언어 모델을 이용하여 실제 자연어처리 문제를 푸는 과정\n","* 실질적으로 성능이 관찰되는 것은 전이학습 이지만, 언어 모델이 제대로 학습되야 전이학습 시 좋은 성능이 나옴\n","* 기존 알고리즘들은 자연어의 다양한 Task에 각각의 알고리즘을 독립적으로 만들어야 했지만, BERT 개발 이후 많은 자연어처리 연구자들은 언어 모델을 만드는데 더 공을 들이게 됨\n","* 전이학습 Task의 성능도 훨씬 더 좋아짐\n","* 전이학습은 라벨이 주어지므로 지도학습(Supervised learning)"]},{"cell_type":"markdown","metadata":{"id":"OyfDPHbkFHgr"},"source":["* 전이학습은 BERT의 언어 모델의 출력에 추가적인 모델을 쌓아서 사용\n","* 일반적으로 복잡한 CNN, LSTM, Attention을 쌓지 않고 간단한 DNN만 쌓아도 성능이 잘 나오며 별 차이가 없다고 알려짐\n","\n","\n","---\n","BERT자체가 성능이 좋아서 다른 거 쌓으면 오히려 더 안좋아질 수도 있음\n","\n","\n","---\n","\n","\n","\n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdHqgat%2Fbtqzl4CSqNd%2F7q3g5hxTcAENvvcu1wK6KK%2Fimg.png)"]},{"cell_type":"markdown","metadata":{"id":"i05t2LQaFjVS"},"source":["## BERT 친구들\n","\n","![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbMaiOM%2FbtqznO6UO3m%2FwvMAVAZDLngmplVbkn0gqK%2Fimg.jpg)"]},{"cell_type":"markdown","metadata":{"id":"4YNQLQHji8lM"},"source":["# BERT 네이버 영화 리뷰 분류\n","\n","* 참고: https://github.com/NLP-kr/tensorflow-ml-nlp-tf2"]},{"cell_type":"markdown","metadata":{"id":"uvIRqh0AIl8d"},"source":["## 라이브러리"]},{"cell_type":"code","metadata":{"id":"KiKkbJmvkd98","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711881883967,"user_tz":-540,"elapsed":29738,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"b916e1a0-539d-4720-8f29-a8537a8d2a46"},"source":["!pip install transformers==2.11.0\n","!pip install tensorflow==2.2.0\n","# 호환성문제로 버전을 맞춰준"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==2.11.0\n","  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m674.8/674.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (1.25.2)\n","Collecting tokenizers==0.7.0 (from transformers==2.11.0)\n","  Downloading tokenizers-0.7.0.tar.gz (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (24.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (3.13.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2.31.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (4.66.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (2023.12.25)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformers==2.11.0) (0.1.99)\n","Collecting sacremoses (from transformers==2.11.0)\n","  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==2.11.0) (2024.2.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->transformers==2.11.0) (1.3.2)\n","Building wheels for collected packages: tokenizers\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n","\u001b[0mFailed to build tokenizers\n","\u001b[31mERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.2.0\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"TAd2vXBshrRh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711881915733,"user_tz":-540,"elapsed":31771,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"53d94d74-0149-4d56-ea0c-939ff44e2746"},"source":["import os\n","import re\n","import json\n","import copy\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn-white')\n","\n","from tqdm import tqdm\n","import tensorflow as tf\n","from transformers import *\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-2-202d5678fa23>:8: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n","  plt.style.use('seaborn-white')\n","/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v4.40. Import as `from transformers import GenerationMixin` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v4.40. Import as `from transformers import TFGenerationMixin` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v4.40. Import as `from transformers import FlaxGenerationMixin` instead.\n","  warnings.warn(\n","No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"]}]},{"cell_type":"markdown","metadata":{"id":"RDRKt69ChrRY"},"source":["## 데이터 다운로드"]},{"cell_type":"code","metadata":{"id":"kP74lUeSIABN","executionInfo":{"status":"ok","timestamp":1711881915734,"user_tz":-540,"elapsed":6,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["tf.random.set_seed(111)\n","np.random.seed(111)\n","\n","BATCH_SIZE = 32\n","NUM_EPOCHS = 3\n","VALID_SPLIT = 0.2\n","MAX_LEN = 39"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OboRn3Vcj_GW"},"source":["* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\n","* https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\n"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"_eD3MbDphrSB","executionInfo":{"status":"ok","timestamp":1711881916685,"user_tz":-540,"elapsed":956,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["import urllib.request\n","\n","train_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\")\n","test_file = urllib.request.urlopen(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\")\n","\n","train_data = pd.read_table(train_file)\n","test_data = pd.read_table(test_file)\n","\n","train_data = train_data.dropna()\n","test_data = test_data.dropna()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_ZCDWgskiRp","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1711881916685,"user_tz":-540,"elapsed":9,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"eb254f92-aa99-4e50-f916-fe96b6588f57"},"source":["train_data.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         id                                           document  label\n","0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n","1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n","2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n","3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n","4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"],"text/html":["\n","  <div id=\"df-7bfc5e3a-d3ae-4254-979e-8d1592cfaa80\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>9976970</td>\n","      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3819312</td>\n","      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>10265843</td>\n","      <td>너무재밓었다그래서보는것을추천한다</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9045019</td>\n","      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6483659</td>\n","      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bfc5e3a-d3ae-4254-979e-8d1592cfaa80')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7bfc5e3a-d3ae-4254-979e-8d1592cfaa80 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7bfc5e3a-d3ae-4254-979e-8d1592cfaa80');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e38ef691-38dd-4f71-8653-459529fe5f95\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e38ef691-38dd-4f71-8653-459529fe5f95')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e38ef691-38dd-4f71-8653-459529fe5f95 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"train_data"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"vVnAFFU-kiny","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1711881917132,"user_tz":-540,"elapsed":452,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"f0843532-6ed2-4318-885c-433d0b0ebf6d"},"source":["test_data.head()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        id                                           document  label\n","0  6270596                                                굳 ㅋ      1\n","1  9274899                               GDNTOPCLASSINTHECLUB      0\n","2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n","3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n","4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"],"text/html":["\n","  <div id=\"df-f156fb0c-ac87-49cf-88ec-db9c0945c00e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6270596</td>\n","      <td>굳 ㅋ</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>9274899</td>\n","      <td>GDNTOPCLASSINTHECLUB</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8544678</td>\n","      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6825595</td>\n","      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6723715</td>\n","      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f156fb0c-ac87-49cf-88ec-db9c0945c00e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f156fb0c-ac87-49cf-88ec-db9c0945c00e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f156fb0c-ac87-49cf-88ec-db9c0945c00e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-862335b2-3d76-400c-bbe9-82166183b6d4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-862335b2-3d76-400c-bbe9-82166183b6d4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-862335b2-3d76-400c-bbe9-82166183b6d4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"test_data","summary":"{\n  \"name\": \"test_data\",\n  \"rows\": 49997,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2936633,\n        \"min\": 601,\n        \"max\": 10278090,\n        \"num_unique_values\": 49997,\n        \"samples\": [\n          5531942,\n          4592330,\n          7415980\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 49157,\n        \"samples\": [\n          \"\\uc18c\\ub9ac\\uc9c0\\ub974\\uc9c0\\ub9c8\\ub77c!!\\uaf2d \\uacf5\\ud3ec\\uc601\\ud654\\uc5d0 \\uc790\\uc2e0\\uc5c6\\ub294 \\uac83\\ub4e4\\uc774 \\uc18c\\ub9ac\\ub9cc \\uc9c0\\ub974\\ub354\\ub77c~\\ud3ec\\uc2a4\\ud130\\uac00 \\ub354\\ubb34\\uc12d\\ub2e4\",\n          \"\\ud734...\\ucc39\\ucc39\\ud558\\ub124..\",\n          \"\\ub300\\ubc15~~~!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"RbG9rFUZkoXv"},"source":["## BertTokenizer"]},{"cell_type":"markdown","metadata":{"id":"UYk5cINxlIcM"},"source":["* 참조: https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus"]},{"cell_type":"code","metadata":{"id":"Ymur-MI3hrSJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711883979407,"user_tz":-540,"elapsed":1530,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"8d3bf13f-450d-42ea-c5e3-c849c54a1c55"},"source":["# BERT 모델을 활용하여 문장을 토큰화하는 함수를 정의\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', cache_dir = 'bert_ckpt', do_lower_case = False)\n","\n","def bert_tokenizer(sentence, MAX_LEN):\n","\n","    encoded_dict = tokenizer.encode_plus(\n","        text = sentence,\n","        add_special_tokens = True,\n","        max_length = MAX_LEN,\n","        pad_to_max_length = True,\n","        return_attention_mask = True\n","    ) # 입력된 문장을 BERT 토크나이저를 사용하여 인코딩합니다. 이때 특별 토큰을 추가하고, 최대 길이에 맞게 패딩을 추가하며, 어텐션 마스크를 반환\n","\n","    input_id = encoded_dict['input_ids']\n","    attention_mask = encoded_dict['attention_mask']\n","    token_type_id = encoded_dict['token_type_ids']\n","\n","    return input_id, attention_mask, token_type_id # 인코딩된 결과에서 입력 토큰 ID, 어텐션 마스크, 토큰 타입 ID를 추출"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/vocab.txt\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer_config.json\n","loading file tokenizer.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer.json\n","loading configuration file config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-multilingual-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n"]}]},{"cell_type":"code","metadata":{"id":"tagwY491hrSO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711884037744,"user_tz":-540,"elapsed":54613,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"97d8ceed-26df-4ceb-fe59-c2627368fd6b"},"source":["# 훈련 데이터를 BERT 모델에 입력으로 사용할 수 있는 형태로 변환하는 작업을 수행\n","input_ids = []\n","attention_masks = []\n","token_type_ids = []\n","train_data_labels = []\n","\n","for train_sentence, train_label in tqdm(zip(train_data['document'], train_data['label']), total = len(train_data)):\n","# 훈련 데이터의 각 문장과 레이블을 반복하며 처리. zip() 함수를 사용하여 문장과 레이블을 하나씩 가져옵니다.\n","\n","    try: # try와 except 블록을 사용하여 예외를 처리. bert_tokenizer() 함수에서 오류가 발생하면 예외를 처리하고 넘어감\n","        input_id, attention_mask, token_type_id = bert_tokenizer(train_sentence, MAX_LEN)\n","\n","        input_ids.append(input_id)\n","        attention_masks.append(attention_mask)\n","        token_type_ids.append(token_type_id)\n","        train_data_labels.append(train_label)\n","    except Exception as e:\n","        print(e)\n","        pass\n","\n","# np.array() 함수를 사용하여 각 리스트를 NumPy 배열로 변환\n","train_movie_input_ids = np.array(input_ids, dtype = int)\n","train_movie_attention_masks = np.array(attention_masks, dtype = int)\n","train_movie_token_type_ids = np.array(token_type_ids, dtype = int)\n","train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_token_type_ids)\n","train_data_labels = np.asarray(train_data_labels, dtype = np.int32)\n","\n","print(\"Sentences : {} \\n Labels: {}\".format(len(train_movie_input_ids), len(train_data_labels)))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/149995 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n","100%|██████████| 149995/149995 [00:53<00:00, 2813.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Sentences : 149995 \n"," Labels: 149995\n"]}]},{"cell_type":"code","metadata":{"id":"ZQJguadKhrSS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711884049208,"user_tz":-540,"elapsed":4,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"2bf2fe5c-072e-4d14-f0c7-000dc0006027"},"source":["# 훈련 데이터에서 특정 인덱스에 해당하는 문장을 선택하고, 해당 문장을 BERT 모델의 입력 형식으로 변환한 후 출력\n","idx = 5\n","\n","input_id = train_movie_input_ids[idx]\n","attention_mask = train_movie_attention_masks[idx]\n","token_type_id = train_movie_token_type_ids[idx]\n","# 선택한 인덱스에 해당하는 훈련 데이터의 문장을 각각의 형식으로 선택\n","# train_movie_input_ids, train_movie_attention_masks, train_movie_token_type_ids는 앞서 생성된 BERT 모델의 입력 형식에 맞게 변환된 데이터\n","\n","print(input_id)\n","print(attention_mask)\n","print(token_type_id)\n","print(tokenizer.decode(input_id))\n","# 5번 인덱스에 대해서 출력한 결과, input_id,attention_mask(마스크 뒷부분은 0으로 채워져 있음), token_type_id는 다 0, input_id에 대해서 decode -> 실제 들어가있는 단어값들이 출력"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[   101   9247   8867  32158  23811    100    124  24982  17655   9757\n","  55511    122  23321  10954  24017  12030    129 106249  24974  30858\n","  18227    119    100    119    119    119   9353  30134  21789  12092\n","   9519 118671 119169    119    102      0      0      0      0]\n","[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n"," 0 0]\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0]\n","[CLS] 막 걸음마 [UNK] 3세부터 초등학교 1학년생인 8살용영화. [UNK]... 별반개도 아까움. [SEP] [PAD] [PAD] [PAD] [PAD]\n"]}]},{"cell_type":"code","metadata":{"id":"PaO33a6ChrSW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711884912133,"user_tz":-540,"elapsed":9925,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"bb1bb442-6ac6-444a-b377-7b2f3fbdc8a3"},"source":["# BERT 모델을 사용하여 텍스트 분류 작업을 수행하는 TensorFlow 모델을 구성\n","class TFBertClassifier(tf.keras.Model): # TFBertClassifier 클래스: 이 클래스는 TensorFlow의 tf.keras.Model을 상속하여 BERT 기반 분류기 모델을 정의\n","    def __init__(self, model_name, dir_path, num_class): # 클래스의 생성자로, 모델의 초기화를 담당합니다. 이 메서드에서는 BERT 모델을 불러오고, 드롭아웃 레이어와 분류를 위한 레이어를 설정\n","        super(TFBertClassifier, self).__init__()\n","\n","        self.bert = TFBertModel.from_pretrained(model_name, cache_dir = dir_path)\n","        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n","        self.classifier = tf.keras.layers.Dense(num_class,\n","                                                kernel_initializer = tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range),\n","                                                name = 'classifier')\n","\n","    def call(self, inputs, attention_mask = None, token_type_ids = None, training = False): # call 메서드: 이 메서드는 모델의 forward pass를 정의\n","        outputs = self.bert(inputs, attention_mask = attention_mask, token_type_ids = token_type_ids)\n","        pooled_output = outputs[1]\n","        pooled_output = self.dropout(pooled_output, training = training)\n","        logits = self.classifier(pooled_output)\n","\n","        return logits # 입력으로는 토큰 ID, 어텐션 마스크, 토큰 타입 ID가 주어지며, BERT 모델을 통해 출력을 계산하고, 드롭아웃을 적용한 후 분류를 위한 레이어에 입력\n","\n","cls_model = TFBertClassifier(model_name = 'bert-base-multilingual-cased',\n","                             dir_path = 'bert_ckpt',\n","                             num_class = 2)\n","# TFBertClassifier 클래스의 인스턴스를 생성합니다. 이 인스턴스는 BERT 모델을 기반으로한 분류기 모델을 나타냄\n","# bert, dropout, classifier는 모델 내에서 사용되는 요소. bert는 사전 학습된 BERT 모델을 의미, dropout은 드롭아웃 레이어, classifier는 분류를 위한 레이어."],"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.38.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading weights file model.safetensors from cache at bert_ckpt/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\n","Loaded 177,853,440 parameters in the TF 2.0 model.\n","Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}]},{"cell_type":"markdown","metadata":{"id":"UfubbrJBxhDa"},"source":["## 모델 학습"]},{"cell_type":"code","metadata":{"id":"Df3MZ75XhrSa","executionInfo":{"status":"ok","timestamp":1711885747616,"user_tz":-540,"elapsed":4,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["# 훈련 프로세스를 설정하는 단계로, 주어진 데이터셋에 대해 모델을 훈련하기 위한 준비를 마친 후 실행\n","optimizer = tf.keras.optimizers.Adam(3e-5) # Adam 옵티마이저를 사용하여 모델을 최적화합니다. 학습률은 3e-5로 설정\n","loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True) # 모델의 손실 함수로는 Sparse Categorical Crossentropy를 사용, 모델의 출력이 로짓이라고 가정\n","metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy') # 모델의 성능을 측정하는 데 사용되는 메트릭으로는 Sparse Categorical Accuracy를 사용. 이 메트릭은 희소한 카테고리(클래스) 예측의 정확도를 측정\n","cls_model.compile(optimizer = optimizer, loss = loss, metrics = [metric]) #  모델을 컴파일. 이 함수는 최적화기(optimizer), 손실(loss), 및 메트릭(metric)을 설정"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvBygAVGhrSf","colab":{"base_uri":"https://localhost:8080/","height":442},"executionInfo":{"status":"error","timestamp":1711892436686,"user_tz":-540,"elapsed":2547395,"user":{"displayName":"이다빈","userId":"05137797752462423239"}},"outputId":"5567340f-064e-460c-8224-49c0beeabaf9"},"source":["model_name = \"tf2_bert_naver_movie\"\n","\n","es_callback = EarlyStopping(monitor = 'val_accuracy', min_delta = 0.0001, patience = 2) # 2번 이상 상승이 있어야함 없으면 종료.\n","\n","checkpoint_path = os.path.join('./', model_name, 'weights.h5')\n","checkpoint_dir = os.path.dirname(checkpoint_path)\n","\n","if os.path.exists(checkpoint_dir):\n","    print(\"{} Directory already exists\\n\".format(checkpoint_dir)) # 이미 존재한다\n","else:\n","    os.makedirs(checkpoint_dir, exist_ok = True)\n","    print(\"{} Directory create complete\\n\".format(checkpoint_dir))\n","\n","cp_callback = ModelCheckpoint(checkpoint_path, monitor = 'val_accuracy',\n","                              verbose = 1, save_best_only = True, save_weights_only = True)\n","# 검증 성능이 개선될 때마다 모델의 가중치를 저장하는 콜백\n","# val_accuracy'를 모니터링하며, 가장 좋은 성능을 보인 모델의 가중치만을 저장\n","\n","history = cls_model.fit(train_movie_inputs, train_data_labels,\n","                        epochs = NUM_EPOCHS, batch_size = BATCH_SIZE, validation_split = VALID_SPLIT,\n","                        callbacks = [es_callback, cp_callback]) # 모델 훈련\n","# 훈련 데이터인 train_movie_inputs와 레이블인 train_data_labels을 사용하며, 에폭 수, 배치 크기, 검증 데이터 분할 비율 등을 지정\n","# 또한 앞에서 설정한 EarlyStopping 콜백과 ModelCheckpoint 콜백을 적용"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["./tf2_bert_naver_movie Directory already exists\n","\n","Epoch 1/3\n"," 433/3750 [==>...........................] - ETA: 12:53:57 - loss: 0.4903 - accuracy: 0.7594"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-cdfaf576f4b9>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# val_accuracy'를 모니터링하며, 가장 좋은 성능을 보인 모델의 가중치만을 저장\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m history = cls_model.fit(train_movie_inputs, train_data_labels,\n\u001b[0m\u001b[1;32m     20\u001b[0m                         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVALID_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                         callbacks = [es_callback, cp_callback]) # 모델 훈련\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"SzUB0CmvhrSh","executionInfo":{"status":"aborted","timestamp":1711882073704,"user_tz":-540,"elapsed":220554,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'], '')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(['loss', 'val_loss'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2C2pRPOxzqm"},"source":["## 모델 평가"]},{"cell_type":"code","metadata":{"id":"6DB2-EymhrSp","executionInfo":{"status":"aborted","timestamp":1711882073705,"user_tz":-540,"elapsed":220554,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["input_ids = []\n","attention_masks = []\n","token_type_ids = []\n","test_data_labels = []\n","\n","for test_sentence, test_label in tqdm(zip(test_data['document'], test_data['label'])):\n","\n","    try:\n","        input_id, attention_mask, token_type_id = bert_tokenizer(test_sentence, MAX_LEN)\n","\n","        input_ids.append(input_id)\n","        attention_masks.append(attention_mask)\n","        token_type_ids.append(token_type_id)\n","        test_data_labels.append(test_label)\n","    except Exception as e:\n","        print(e)\n","        pass\n","\n","test_movie_input_ids = np.array(input_ids, dtype = int)\n","test_movie_attention_masks = np.array(attention_masks, dtype = int)\n","test_movie_token_type_ids = np.array(token_type_ids, dtype = int)\n","test_movie_inputs = (test_movie_input_ids, test_movie_attention_masks, test_movie_token_type_ids)\n","test_data_labels = np.asarray(test_data_labels, dtype = np.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLu7M6TohrSt","executionInfo":{"status":"aborted","timestamp":1711882073705,"user_tz":-540,"elapsed":220553,"user":{"displayName":"이다빈","userId":"05137797752462423239"}}},"source":["cls_model.evaluate(test_movie_inputs, test_data_labels, batch_size = 1024)"],"execution_count":null,"outputs":[]}]}