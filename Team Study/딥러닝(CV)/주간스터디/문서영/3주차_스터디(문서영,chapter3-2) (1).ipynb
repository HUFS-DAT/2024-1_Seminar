{"cells":[{"cell_type":"markdown","metadata":{"id":"6u_NNpzby1dj"},"source":["### 데이터 다운로드"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QR9pNZl6y1dl","executionInfo":{"status":"ok","timestamp":1710992036843,"user_tz":-540,"elapsed":891,"user":{"displayName":"문서영","userId":"04624171365240910342"}},"outputId":"83e7af3c-29ee-4055-d8ac-c48a61cd5f84"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-03-21 03:33:55--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.162.207, 74.125.134.207, 74.125.141.207, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.162.207|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 68606236 (65M) [application/zip]\n","Saving to: ‘cats_and_dogs_filtered.zip’\n","\n","cats_and_dogs_filte 100%[===================>]  65.43M   129MB/s    in 0.5s    \n","\n","2024-03-21 03:33:56 (129 MB/s) - ‘cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n","\n"]}],"source":["!wget https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rs3eEq8my1dl"},"outputs":[],"source":["!unzip -qq cats_and_dogs_filtered.zip"]},{"cell_type":"code","source":["ls"],"metadata":{"id":"utG93o9pNb5S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"IrMV7hY8NZtX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_hzyim-_y1dm"},"outputs":[],"source":["# 패키지 임포트\n","import os\n","from torch.utils.data import Dataset\n","import torchvision.transforms as transforms\n","from PIL import Image"]},{"cell_type":"markdown","source":["os 모듈은 파일 시스템에서 파일을 생성, 삭제, 수정하거나, 파일 경로를 조작하고, 환경 변수에 접근하는 등의 작업 수행"],"metadata":{"id":"5jrgR6GAOKyM"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4mhpPIky1dm","executionInfo":{"status":"ok","timestamp":1710992314426,"user_tz":-540,"elapsed":1071,"user":{"displayName":"문서영","userId":"04624171365240910342"}},"outputId":"4ad067e9-803b-473a-e64a-cd722a9d54ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.2.1+cu121\n"]}],"source":["#pytorch 버전 확인\n","import torch\n","print(torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qT5CeTVpy1dm"},"outputs":[],"source":["# GPU 사용 체크\n","is_cuda = False\n","if torch.cuda.is_available():\n","    is_cuda = True"]},{"cell_type":"markdown","metadata":{"id":"nSN8LIMSy1dn"},"source":["### 파이토치 데이터셋 클래스"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0yRyZWmhy1dn","executionInfo":{"status":"ok","timestamp":1710992320904,"user_tz":-540,"elapsed":3,"user":{"displayName":"문서영","userId":"04624171365240910342"}},"outputId":"e70ca156-3947-40ab-b820-60732e2b320c"},"outputs":[{"output_type":"stream","name":"stderr","text":["<>:25: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n","<>:25: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n","<ipython-input-7-6491a468c92d>:25: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n","  if image.mode is not \"RGB\":\n"]}],"source":["# 데이터세트 정의 클래스\n","class PyTorchCustomDataset(Dataset):\n","    def __init__(self\n","                 , root_dir = \"/content/cats_and_dogs_filtered/train\"\n","                 , transform = None):\n","        self.image_abs_path = root_dir\n","        self.transform = transform\n","        self.label_list = os.listdir(self.image_abs_path)\n","        self.label_list.sort()\n","        self.x_list = []\n","        self.y_list = []\n","        for label_index, label_str in enumerate(self.label_list):\n","            img_path = os.path.join(self.image_abs_path, label_str)\n","            img_list = os.listdir(img_path)\n","            for img in img_list:\n","                self.x_list.append(os.path.join(img_path, img))\n","                self.y_list.append(label_index)\n","        pass\n","\n","    def __len__(self):\n","        return len(self.x_list)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.x_list[idx])\n","        if image.mode is not \"RGB\":\n","            image = image.convert('RGB')\n","        if self.transform is not None:\n","            image = self.transform(image)\n","        return image, self.y_list[idx]\n","\n","    def __save_label_map__(self, dst_text_path = \"label_map.txt\"):\n","        label_list = self.label_list\n","        f = open(dst_text_path, 'w')\n","        for i in range(len(label_list)):\n","            f.write(label_list[i]+'\\n')\n","        f.close()\n","        pass\n","\n","    def __num_classes__(self):\n","        return len(self.label_list)"]},{"cell_type":"markdown","source":["__init__ 메서드는 클래스의 인스턴스가 생성될 때 초기화를 담당\n","\n","root_dir 매개변수는 이미지 파일이 위치한 기본 경로를 설정\n","\n","transform 매개변수는 이미지에 적용할 전처리(transform)를 정의\n","\n","self.image_abs_path는 이미지 파일의 절대 경로를 저장\n","\n","self.label_list는 root_dir 경로에 있는 각 레이블(디렉토리)의 목록을 저장\n","\n","self.x_list와 self.y_list는 각각 이미지 파일의 경로와 해당 이미지의 레이블 인덱스를 저장\n","\n","모든 레이블에 대해 이미지 경로와 레이블 인덱스를 self.x_list와 self.y_list에 추가"],"metadata":{"id":"eFlbi42XOTwC"}},{"cell_type":"markdown","source":["__len__ 메서드는 데이터셋의 총 아이템 수를 반환. 이 경우 self.x_list의 길이를 반환"],"metadata":{"id":"vhZ0B-wKOwB9"}},{"cell_type":"markdown","source":["__getitem__ 메서드는 주어진 인덱스(idx)에 해당하는 아이템(이미지와 레이블)을 데이터셋에서 가져온다.\n","\n","이미지 파일은 Image.open을 사용하여 열리며, 이미지가 RGB 모드가 아닌 경우 RGB로 변환\n","\n","설정된 transform이 있다면 이미지에 적용\n","\n","변환된 이미지와 해당 레이블 인덱스가 반환"],"metadata":{"id":"up7h5kp_Oyok"}},{"cell_type":"markdown","source":["__save_label_map__ 메서드는 레이블 목록을 텍스트 파일로 저장\n","\n","파일 경로는 dst_text_path 매개변수를 통해 지정\n","\n","각 레이블을 줄 단위로 파일에 쓰고 파일을 닫는다."],"metadata":{"id":"iLBD-vBJO8Yi"}},{"cell_type":"markdown","source":["__num_classes__ 메서드는 데이터셋의 클래스(레이블) 수를 반환, self.label_list의 길이와 동일"],"metadata":{"id":"op6MzzWGPIkj"}},{"cell_type":"markdown","metadata":{"id":"7P-86gI4y1dn"},"source":["### model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnsUXujSy1dn"},"outputs":[],"source":["# 네트워크 정의\n","import torch\n","from torchvision import models\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MODEL(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","        self.network = models.resnet18(pretrained=True)\n","        self.classifier = nn.Sequential(\n","            nn.Dropout()\n","            , nn.Linear(1000, num_classes)\n","            , nn.Sigmoid()\n","        )\n","    def forward(self, x):\n","        x = self.network(x)\n","        return self.classifier(x)"]},{"cell_type":"markdown","source":["사전 학습된 ResNet-18 아키텍처를 기반\n","\n","self.network는 사전 학습된 ResNet-18 모델을 로드\n","\n","pretrained=True 인자는 사전 학습된 가중치를 사용하여 모델을 초기화\n","\n","self.classifier는 신경망의 마지막 부분을 사용자 정의 분류기로 대체\n","\n","nn.Dropout(): 과적합을 방지하기 위해 드롭아웃을 적용. 드롭아웃 비율은 기본값(0.5)을 사용\n","\n","nn.Linear(1000, num_classes): 선형 레이어(완전 연결 레이어)로, ResNet-18의 마지막 레이어 출력(기본적으로 1000차원)을 num_classes 차원으로 변환\n","\n","nn.Sigmoid(): 이진 분류 문제를 위한 활성화 함수로 시그모이드 함수, 다중 레이블 분류 문제에 적합하며, 각 클래스에 대한 독립적인 확률을 출력\n","\n","forward 메서드는 모델이 입력 데이터 x를 받아 출력을 계산하는 방식"],"metadata":{"id":"K0uKbDwVPX6J"}},{"cell_type":"markdown","metadata":{"id":"fCmjPOL0y1do"},"source":["### main 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vHBYayGay1do"},"outputs":[],"source":["# 훈련 메인 함수 정의\n","import torch\n","import torch.optim as optim\n","\n","train_losses , train_accuracy = [],[]\n","val_losses , val_accuracy = [],[]\n","\n","def trainmain():\n","    USE_CUDA = torch.cuda.is_available()\n","    DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","    img_width, img_height = 224, 224\n","    EPOCHS     = 12\n","    BATCH_SIZE = 32\n","\n","    #데이터세트 로딩\n","    transform_train = transforms.Compose([\n","                transforms.Resize(size=(img_width, img_height))\n","                , transforms.RandomRotation(degrees=15)\n","                , transforms.ToTensor()\n","                ])\n","    transform_test = transforms.Compose([\n","                transforms.Resize(size=(img_width, img_height))\n","                , transforms.ToTensor()\n","                ])\n","    TrainDataset = PyTorchCustomDataset\n","    TestDataset = PyTorchCustomDataset\n","    train_data = TrainDataset(root_dir = \"/content/cats_and_dogs_filtered/train\"\n","                    , transform = transform_train)\n","    test_data = TestDataset(root_dir = \"/content/cats_and_dogs_filtered/validation\"\n","                    , transform = transform_test)\n","    train_loader = torch.utils.data.DataLoader(\n","        train_data\n","        , batch_size=BATCH_SIZE\n","        , shuffle=True\n","    )\n","    test_loader = torch.utils.data.DataLoader(\n","        test_data\n","        , batch_size=BATCH_SIZE\n","        , shuffle=True\n","    )\n","    train_data.__save_label_map__()\n","    num_classes = train_data.__num_classes__()\n","\n","    #모델 객체 생성, PyTorch_Classification_Model.pt 모델 파일명 지정\n","    model = MODEL(num_classes).to(DEVICE)\n","    model_str = \"PyTorch_Classification_Model\"\n","    model_str += \".pt\"\n","\n","    #최적화 함수와 학습률 지정\n","    #optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","    #scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n","    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","    acc = 0.0\n","\n","    # 에포크 만큼 훈련, 검증\n","    for epoch in range(1, EPOCHS + 1):\n","        model.train()\n","        tr_loss = 0.0\n","        tr_correct = 0.0\n","        for data, target in (train_loader):\n","            data, target = data.to(DEVICE), target.to(DEVICE)\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = F.cross_entropy(output, target)\n","            tr_loss += F.nll_loss(output,target,reduction='sum').item()\n","            pred = output.data.max(dim=1,keepdim=True)[1]\n","            tr_correct += pred.eq(target.view_as(pred)).sum().item()\n","            loss.backward()\n","            optimizer.step()\n","        scheduler.step()\n","        tr_ep_loss = tr_loss/len(train_loader.dataset)\n","        tr_ep_accuracy = 100. * tr_correct/len(train_loader.dataset)\n","\n","        model.eval()\n","        te_loss = 0\n","        te_correct = 0\n","        with torch.no_grad():\n","            for data, target in (test_loader):\n","                data, target = data.to(DEVICE), target.to(DEVICE)\n","                output = model(data)\n","                loss = F.cross_entropy(output, target)\n","                te_loss += F.cross_entropy(output, target, reduction='sum').item()\n","                pred = output.max(1, keepdim=True)[1]\n","                te_correct += pred.eq(target.view_as(pred)).sum().item()\n","        te_ep_loss = te_loss / len(test_loader.dataset)\n","        te_ep_accuracy = 100. * te_correct / len(test_loader.dataset)\n","        print('[{}] Train Loss: {:.4f}, Train Accuracy: {:.2f}% Test Loss: {:.4f}, Test Accuracy: {:.2f}%'.format(\n","                epoch, tr_ep_loss, tr_ep_accuracy, te_ep_loss, te_ep_accuracy))\n","\n","        if acc < te_ep_accuracy:\n","            acc = te_ep_accuracy\n","            torch.save(model.state_dict(), model_str)\n","            print(\"model saved!\")\n","\n","        train_losses.append(tr_ep_loss)\n","        train_accuracy.append(tr_ep_accuracy)\n","        val_losses.append(te_ep_loss)\n","        val_accuracy.append(te_ep_accuracy)"]},{"cell_type":"markdown","source":["이미지의 너비와 높이를 224로 설정하고, 한 번에 처리할 이미지 수인 배치 크기를 32로 설정\n","\n","훈련 데이터에는 크기 조정, 무작위 회전, Tensor 변환을 적용하고, 테스트 데이터에는 크기 조정과 Tensor 변환을 적용\n","\n","데이터 로더는 배치 크기에 맞게 데이터를 나누고, 훈련 데이터의 경우 무작위로 섞는다.\n","\n","Adam 최적화 함수를 사용하고, 학습률 스케줄러를 설정하여 훈련 과정에서 학습률을 조정\n","\n","각 에포크마다 훈련 및 검증 데이터셋에 대한 손실(loss)과 정확도(accuracy)를 계산\n","\n","역전파를 통해 모델의 가중치를 업데이트\n","\n","기울기 계산이 필요 없으므로 torch.no_grad()를 사용"],"metadata":{"id":"ZVuuZYfhQpqM"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5e_OlNq-y1dp","outputId":"4667a8b7-9a25-4510-d639-c184a64b7916"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 108MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[1] Train Loss: -0.9026, Train Accuracy: 93.95% Test Loss: 0.3490, Test Accuracy: 96.50%\n","model saved!\n","[2] Train Loss: -0.9747, Train Accuracy: 98.15% Test Loss: 0.3336, Test Accuracy: 98.10%\n","model saved!\n","[3] Train Loss: -0.9823, Train Accuracy: 98.65% Test Loss: 0.3343, Test Accuracy: 97.90%\n","[4] Train Loss: -0.9876, Train Accuracy: 99.15% Test Loss: 0.3374, Test Accuracy: 97.40%\n","[5] Train Loss: -0.9860, Train Accuracy: 98.75% Test Loss: 0.3382, Test Accuracy: 97.20%\n","[6] Train Loss: -0.9926, Train Accuracy: 99.35% Test Loss: 0.3333, Test Accuracy: 98.00%\n","[7] Train Loss: -0.9929, Train Accuracy: 99.50% Test Loss: 0.3336, Test Accuracy: 97.90%\n"]}],"source":["# 훈련 메인 함수 호출\n","trainmain()"]},{"cell_type":"markdown","source":["실제로 이미지 분류 모델의 훈련 및 검증 과정이 시작\n","\n","train epoch 2까지만 돌고 훈련 진행이 안됨...ㅠ"],"metadata":{"id":"HXUM5oVlQd1x"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"krrIQcZcy1dp"},"outputs":[],"source":["# 훈련 데이터와 검증 데이터의 손실 그래프\n","import matplotlib.pyplot as plt\n","\n","plt.plot(range(1,len(train_losses)+1),train_losses,'bo',label = 'training loss')\n","plt.plot(range(1,len(val_losses)+1),val_losses,'r',label = 'validation loss')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ou_H-yTay1dp"},"outputs":[],"source":["# 훈련 데이터와 검증 데이터의 정확도 그래프\n","plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'bo',label = 'train accuracy')\n","plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\n","plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6sIMlsNy1dp"},"outputs":[],"source":["# PyTorch_Classification_Model.pt 모델 파일 확인\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"gujOS0Kry1dp"},"source":["### 이미지 분류 모델 추론 테스트"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aHTiqbSiy1dq"},"outputs":[],"source":["# 테스트 이미지 로딩\n","import os\n","PATH = \"/content/cats_and_dogs_filtered/validation\"\n","validation_cats_dir = PATH + '/cats'  # directory with our validation cat pictures\n","validation_dogs_dir = PATH + '/dogs'  # directory with our validation dog pictures\n","list_of_test_cats_images = os.listdir(validation_cats_dir)\n","list_of_test_dogs_images = os.listdir(validation_dogs_dir)\n","for idx in range(len(list_of_test_cats_images)):\n","    list_of_test_cats_images[idx] = validation_cats_dir + '/'+list_of_test_cats_images[idx]\n","for idx in range(len(list_of_test_dogs_images)):\n","    list_of_test_dogs_images[idx] = validation_dogs_dir + '/'+list_of_test_dogs_images[idx]\n","list_of_test_images = list_of_test_cats_images + list_of_test_dogs_images"]},{"cell_type":"markdown","source":["특정 디렉토리(여기서는 고양이와 개 사진이 있는 검증 데이터셋 디렉토리) 내의 이미지 파일 목록을 생성하고, 각 파일의 전체 경로를 리스트에 저장하는 과정\n","\n","os.listdir 함수를 사용하여 validation_cats_dir와 validation_dogs_dir 디렉토리 내의 모든 파일(이 경우 이미지) 목록을 각각 가져온다.\n","\n","각 파일명 앞에 해당 파일의 디렉토리 경로를 추가함으로써 전체 파일 경로로 업데이트. 이는 파일을 접근할 때 필요한 전체 경로를 형성\n","\n","고양이와 개 이미지 파일의 전체 경로를 포함하는 두 리스트를 합쳐서 list_of_test_images라는 하나의 리스트를 생성"],"metadata":{"id":"JfQnhGwdTk3I"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jz1lbf-Cy1dq"},"outputs":[],"source":["# 로딩된 이미지 경로 프린트\n","print(list_of_test_cats_images[10])\n","print(list_of_test_images[501])"]},{"cell_type":"markdown","source":["먼저 고양이 이미지 파일 목록이 추가되고 그 다음에 개 이미지 파일 목록이 추가되므로, 501번째 인덱스에 있는 파일이 고양이인지 개인지는 고양이 이미지와 개 이미지의 전체 수에 따라 달라진다.\n","\n","첫번째가 고양이 그림, 두번째가 강아지 그림이 나온다."],"metadata":{"id":"bsC_kQ7dUO_s"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaiaL-0-y1dq"},"outputs":[],"source":["# 이미지 보여주는 함수, 이미지 추론 함수\n","\n","from PIL import Image\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","#라벨맵 로딩 함수\n","def load_label_map(textFile):\n","    return np.loadtxt(textFile, str, delimiter='\\t')\n","\n","#이미지 읽는 함수\n","def cv_image_read(image_path):\n","    print(image_path)\n","    return cv2.imread(image_path)\n","\n","#이미지 보여주는 함수\n","def show_image(cv_image):\n","    rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n","    plt.figure()\n","    plt.imshow(rgb)\n","    plt.show()\n","\n","#이미지를 분류 모델로 추론한 결과를 텍스트로 보여주는 함수\n","def print_result(inference_result, class_map):\n","    class_text = class_map[np.argmax(inference_result)]\n","    print(inference_result)\n","    print(class_text)\n","\n","#이미지를 분류 모델로 추론하는 함수\n","def inference_image(opencv_image, transform_info, model, DEVICE):\n","    image = Image.fromarray(opencv_image)\n","    image_tensor = transform_info(image)\n","    image_tensor = image_tensor.unsqueeze(0)\n","    image_tensor = image_tensor.to(DEVICE)\n","    result = model(image_tensor)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9bL76lBKy1dq"},"outputs":[],"source":["# 이미지 보여주는 함수 실습\n","show_image(cv_image_read(list_of_test_images[10]))\n","show_image(cv_image_read(list_of_test_images[501]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YL0LwGkQy1dq"},"outputs":[],"source":["# 테스트 메인 함수\n","import os\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import numpy as np\n","\n","def testmain(image_path):\n","    USE_CUDA = torch.cuda.is_available()\n","    DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n","\n","    img_width, img_height = 224, 224\n","    transform_info = transforms.Compose([\n","                transforms.Resize(size=(img_width, img_height))\n","                , transforms.ToTensor()\n","                    ])\n","    #라벨 파일 읽기\n","    class_map = load_label_map('label_map.txt')\n","    num_classes = len(class_map)\n","\n","    #지정된 모델 로딩\n","    model = MODEL(num_classes).to(DEVICE)\n","    model_str = \"PyTorch_Classification_Model\"\n","    model_str += \".pt\"\n","\n","    model.load_state_dict(torch.load(model_str))\n","    model.eval()\n","\n","    #image_path = list_of_test_images[501]\n","    opencv_image = cv_image_read(image_path)\n","    inference_result = inference_image(opencv_image, transform_info, model, DEVICE)\n","    inference_result = inference_result.cpu().detach().numpy()\n","    print_result(inference_result, class_map)\n","    show_image(opencv_image)"]},{"cell_type":"markdown","source":["테스트 함수는 추론을 위한 라벨 파일을 읽고 모델 파일을 로딩한 후, GPU 사용과 입력 이미지를 224X224로 변환을 지정해 이미지를 추론"],"metadata":{"id":"LpcpmLHVVBdC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EYp9LrdYy1ds"},"outputs":[],"source":["# 테스트 이미지로 테스트 메인 함수 실행 1\n","image_path = list_of_test_images[10]\n","testmain(image_path)\n","\n","# 테스트 이미지로 테스트 메인 함수 실행 2\n","image_path = list_of_test_images[501]\n","testmain(image_path)"]},{"cell_type":"markdown","source":["추론 결과를 라벨과 이미지를 화면에 표시"],"metadata":{"id":"BB8hvRXmVU3f"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}